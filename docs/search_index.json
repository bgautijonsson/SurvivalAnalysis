[
["index.html", "Lecture Notes on Survival Analysis Course taught at the University of Iceland Preface The Book Software Setup in RStudio Some Probability Distributions", " Lecture Notes on Survival Analysis Course taught at the University of Iceland Thor Aspelund Brynjólfur Gauti Jónsson 2020-03-07 Preface The Book These lecture notes are based on the book Modelling Survival Data in Medical Research by David Collett Software We will primarily be writing R code using the RStudio GUI. Setup in RStudio This is our base setup. Further packages will be loaded in the individual lectures respectively. library(tidyverse); library(kableExtra); library(cowplot); library(scales); library(janitor); theme_set(theme_cowplot() + background_grid(major = &quot;xy&quot;, minor = &quot;xy&quot;, size.major = 0.25, size.minor = 0.1) + theme(legend.position = &quot;top&quot;, plot.margin = margin(10, 10, 10, 10))) Some Probability Distributions Discrete Bernoulli A random variable \\(X\\) is said to have the Bernoulli distribution with parameter \\(p\\) if \\[ \\begin{aligned} P(X=1) &amp;= p, \\quad \\text{and} \\\\ P(X=0) &amp;= 1 - p, \\quad \\text{where } 0 &lt; p &lt; 1. \\end{aligned} \\] We write this as \\(X \\sim \\mathrm{Bern}(p)\\). All random variables whose possible values are \\(0\\) and \\(1\\) have a this distribution. p &lt;- 0.3 success &lt;- p failure &lt;- 1 - p tibble(outcome = c(&quot;Success&quot;, &quot;Failure&quot;), probability = c(success, failure)) %&gt;% ggplot(aes(outcome, probability)) + geom_col() + scale_y_continuous(labels = percent) + labs(title = &quot;Example Bernoulli variable&quot;) + theme(axis.title = element_blank()) Binomial Suppose that we have n independent Bernoulli random variables sampled from an experiment, each with the same probability, \\(p\\), of success. Let \\(X\\) be the number of successes. Then \\(X\\) has a Binomial distribution with parameters \\(n\\) and \\(p\\). We write \\(X \\sim \\mathrm{Bin}(n, p)\\). Thus the \\(\\mathrm{Bern}(p)\\) distribution is the same as \\(\\mathrm{Bin}(1, p)\\). coin_tosses &lt;- 100 possible_outcomes &lt;- seq(0, 100) probability_of_outcomes &lt;- dbinom(x = possible_outcomes, size = coin_tosses, prob = 0.5) tibble(outcome = possible_outcomes, probability = probability_of_outcomes) %&gt;% ggplot(aes(outcome, probability)) + geom_col() + scale_y_continuous(labels = percent) + labs(title = &quot;Coin toss experiment&quot;, subtitle = &quot;Probability of obtaining X heads in 100 tosses&quot;) + theme(axis.title = element_blank()) Geometric Conside a sequence of independent Bernoulli trials, each with th same success probability, \\(p\\), and imagine performing trials untill a success occurs. Let \\(X\\) be the number of failures before the first successful trial. Then \\(X\\) has the Geometric distribution with parameter \\(p\\), denoted by \\(X \\sim \\mathrm{Geom}(p)\\). This is an important distribution for survival analysis. Imagine that on each day there is a probability, \\(p\\), which remains constant. Then the number of days before the event have a \\(\\mathrm{Geom}(p)\\) distribution. Textbooks and statistical software differ in whether the Geometric distribution models the trials before the success or the number of trials including the success. The number of trials including the success is sometimes called the First Success distribution. risk &lt;- 0.05 possible_times &lt;- seq(0, 100) probability_of_times &lt;- dgeom(possible_times, risk) tibble(time = possible_times, probability = probability_of_times) %&gt;% ggplot(aes(time, probability)) + geom_col() + scale_y_continuous(labels = percent) + labs(title = &quot;Geometic experiment&quot;, subtitle = &quot;Probability of waiting X days untill event happens with risk 5%&quot;) + theme(axis.title = element_blank()) Continuous Exponential The Exponential distribution is the continuous counterpart to the Geometric distribution. Instead of waiting for events in discrete days we are now waiting in continuous time for successes that occurs with rate \\(\\lambda\\) per unit of time. The average number of successes in a time inteval of length \\(t\\) is \\(\\lambda t\\), though the actual number of successes varies. If \\(X\\) has an Exponential distribution we write \\(X \\sim \\mathrm{Expo}(\\lambda)\\), and the probability that an event occures within \\(x\\) units of time is \\[ P(X \\leq x) =F(x) = 1 - e^{-\\lambda x}. \\] In survival snalysis this is often called the risk function. The survivor function is the probability that an event has not occurred within \\(x\\) units of time, and for an Exponential random variable it is written \\[ P(X &gt; x) = S(x) = 1 - (1 - e^{-\\lambda x}) = e^{-\\lambda x}. \\] The Exponential distribution is unique in that it is memoryless. This means that the time untill an event occurs is independent of how long you have been waiting for it to happen. It is obvious that the time untill your bus arrives is not exponentially distributed since the waiting time is not the same whether you have waited \\(0\\), \\(5\\) or \\(15\\) minutes (although sometimes it may feel that way!). Waiting times that are not memoryless are better modelled with other distributions. Weibull Remarkably, simply raising an exponential random variable to a power dramatically improves the flexibility and applicability of the distribution. Let \\(X \\sim \\mathrm{Expo}(\\lambda)\\) and \\(T = X^{1\\gamma}\\) with \\(\\lambda &gt; 0\\), \\(\\gamma &gt; 0\\). The distribution of \\(T\\) is called the Weibull distribution and we denote this by \\(T \\sim \\mathrm{Weib}(\\lambda, \\gamma)\\). We see that if \\(\\gamma = 1\\) then \\(T\\sim \\mathrm{Weib}(\\lambda, 1) = \\mathrm{Expo}(\\lambda)\\). The risk function for a Weibull random variable is \\[ P(T \\leq t) = F(t) = 1 - e^{\\lambda t^{\\gamma}} \\] and the survivor function is \\[ P(T &gt; t) = S(t) = e^{\\lambda t^\\gamma}. \\] The Weibull distribution is not memoryless. "],
["introduction-to-survival-analysis.html", "Chapter 1 Introduction to Survival Analysis 1.1 Inngangur 1.2 Skerðing (censoring) 1.3 Kaplan Meier metillinn 1.4 Miðgildi lifunar og logrank próf 1.5 Víti að varast og breyting á tímaskala", " Chapter 1 Introduction to Survival Analysis library(survival) library(tidyverse) library(epitools) library(survminer) # Gögnin fylgja með epitools pakkanum. Þá er nóg að virkja gögnin með data() skipun. data(wcgs) wcgs$behpat0f &lt;- factor(wcgs$behpat0,levels=1:4,label=c(&quot;A1&quot;,&quot;A2&quot;,&quot;B1&quot;,&quot;B2&quot;)) wcgs$dibpat0f &lt;- factor(wcgs$dibpat0,levels=0:1,label=c(&quot;B&quot;,&quot;A&quot;)) wcgs$smoker &lt;- ifelse(wcgs$ncigs0&gt;0,1,0) wcgs$smokerf &lt;- factor(wcgs$smoker,levels=c(0,1),labels=c(&quot;No&quot;,&quot;Yes&quot;)) wcgs$heightcm &lt;- wcgs$height0*2.54 wcgs$weightkg &lt;- wcgs$weight0*0.45359237 wcgs$bmi &lt;- wcgs$weightkg / (wcgs$heightcm/100)^2 wcgs$cholmmol = wcgs$chol/39 wcgs$time169y &lt;- wcgs$time169/365.24 # restricted followup to 5 years wcgs$time169y5 &lt;- pmin(wcgs$time169y,5) wcgs$chd695 &lt;- (wcgs$chd69==1 &amp; wcgs$time169y &lt;=5) 1.1 Inngangur Efni dagsins í dag er myndræn framsetning i lifunargreiningu. Við munum skipta umfjölluninni í þrennt. Fyrst fjöllum við um skerðingu (e. censoring) af hverju hún stafar og hvernig við vinnum með hana. Því næst fjöllum við um Kaplan Meier metlinn, teiknum Kaplan Meier ferla og lærum að túlka þá og meta. Að lokum skoðum við hvernig meta má miðgildi lifunar (e. median survival) og skoðum tölfræðipróf til að kanna mun á lifunarferlum. Við notum gagnasafnið wcgs sem fylgir t.d. með epitools pakkanum. WCGS stendur fyrir Western Collaborative Group Study. Nánari umfjöllun má sjá á ClincialTrials.gov. Í stuttu máli var 3154 karlmönnum á aldrinum 39 til 59 fylgt eftir í allt að 10 ár þangað til þeir fengu hjartasjúkdóm, eða létust, eða þeir urðu 70 ára eða eftirfylgni lauk af öðrum ástæðum. 1.2 Skerðing (censoring) Meginmarkmið WCGS rannsóknarinnar var að skoða áhættuþætti fyrir hjartasjúkdómum og því eru hjartasjúkdómar aðalútkomubreytan okkar. Breytan chd69 segir til um hvort karlmennirnir fengu hjartasjúkdóm eða ekki. table(wcgs$chd69) ## ## 0 1 ## 2897 257 Við sjáum að alls fengu 257 karlmenn fengu hjartasjúkdóm á meðan rannsókninni stóð. Í lógistískri aðhvarfsgreiningu hefði okkur nægt að líta á þessa breytu sem útkomubreytuna okkar og látið þar við sitja. En fleiri upplýsingar er að finna i gögnunum okkar og því getum við gert betur. Í fyrsta lagi höfum við upplýsingar um nákvæman dagafjölda frá því að eftirfylgdin hófst og þangað til maðurinn fékk hjartasjúkdóm og við getum tekið tillit til þess tíma í líkönunum okkar. Þannig getum við til dæmis skoðað þá sem fengu hjartasjúkdóm innan mjög skamms tíma. Í öðru lagi er meginþorri gagnanna okkar skertur (e. censored), þ.e.a.s. eftirfylgdin stóð einungis yfir í takmarkaðan tíma og því höfum við ekki upplýsingar um hvort sumir mannanna hefðu mögulega þróað með sér hjartasjúkdóm eftir að eftirfylgdinni lauk. Skerðing getur verið af ýmsum ástæðum, maðurinn neitaði áframhaldandi þátttöku, rannsókninni lauk eða þá að hann dó af öðrum orsökum og hafði því ekki möguleika á að þróa með sér hjartasjúkdóm í framhaldinu. Til eru aðferðir til að skoða margs konar útkomur samtímis (t.d. bæði hjartaáföll og dauða) og er þá talað um samkeppni áhættu (e. competing risk). Þær aðferðir eru utan efni þessa námskeiðs. Skerðing getur verið frá vinstri (þ.e.a.s. eftirfylgni hófst ekki fyrr en á ákveðnum tímapunkti) eða frá hægri (eftirfylgni lauk á ákveðnum tímapunkti) eða á bili (eftirfylgni var eingöngu á ákveðnu tímabili). Í langflestum tilvikum er unnið með skerðingu frá hægri og því munum við bara fjalla um hana hér. Á sama hátt og viðbótarupplýsingar eru fólgnar í því hversu fljótt einstaklingur fékk hjartasjúkdóm eru einnig heilmiklar upplýsingar fólgnar i því hversu lengi honum var fylgt eftir án þess að hann fengi hjartasjúkdóm. Því skiptir tími fram að skerðingu jafn miklu máli og tími fram að hjartasjúkdómi. Í R tvinnum við saman þessar upplýsingar með því að búa til breytu af gerðinni lifunarhlutur (e. survival object). Við búum hann til með skipuninni Surv sem er í pakkanum survival og mötum á tveimur breytum: Hve langur tími leið fram að atburði/þangað til eftirfylgd lauk Hvort atburður eða skerðing átti sér stað á þeim tímapunkti. lifunar.hlutur &lt;- Surv(wcgs$time169, wcgs$chd69) Skoðum nánar fyrstu sex mælingarnar í lifunarhlutnum: head(lifunar.hlutur) ## [1] 1664+ 3071+ 3071+ 3064+ 1885 3102+ Fyrsta mælingin er 1664, 0. Plúsinn gefur til kynna að honum var fylgt eftir í 1664 daga án þess að til atburðar kæmi og því er mælingin skert. Fimmta mælinginin, 1885, 1 hefur hins vegar engan plús sem gefur til kynna að maðurinn fékk hjartasjúkdóm eftir 1885, 1 daga. 1.3 Kaplan Meier metillinn Kaplan Meier metilinn er ein langmest notaða tölfræðiaðferðin þegar framkvæmd er lifunargreining, meðal annars vegna þess að á honum byggja svo kölluð Kaplan-Meier gröf sem eru ein algengasta leiðin til að lýsa lifunargögnum. Okkur þykir því ástæða til að skoða hann í kjölinn. Fyrst skulum við kynna til sögunnar svokallað lifunarfall (e. survival curve). Lifunarfallið er yfirleitt táknað með \\(S(t)\\) og túlkun fallsins er svohljóðandi \\[ S(t) = \\text{Líkur þess að einstaklingur fá ekki atburð fram að tíma $t$.} \\] Lifunarfallið miðar alltaf við einhvern tiltekinn atburð. Oft er sá atburður dauði, en hann er einnig mjög oft eitthvað annað eins og til dæmis greining á sjúkdómi. Í því tilviki er stundum talað um “sjúkdómslausa lifun” eða “atburðarlausa lifun” til að skerpa á að lifunin eigi ekki við um líf og dauða, heldur lifun án þess að hafa fengið tiltekinn atburð, sem í þessu tilviki er greining á sjúkdóm. Lifunarfallið gefur okkur þá líkurnar á því að hafa ekki fengið sjúkdóm innan tiltekins tíma. Í okkar tilfelli væri \\(S(365)\\) líkur þess að einstaklingur fái ekki hjartasjúkdóm innan 365 daga. Ef ekki væri fyrir skerðingu væri lítill vandi að meta hvert það gildi væri, sem við samkvæmt venju köllum \\(\\hat S(365)\\). Besta matið á \\(\\hat S(365)\\) væri einfaldlega fjöldi þeirra sem fengu hjartasjúkdóm innan 365 daga af allri heildinni. En skerðingarinnar vegna vandast málin. Kaplan Meier metilinn er stærðfræðileg regla til að meta \\(\\hat S(t)\\). Hann er metinn á sérhverjum tímapunkti þegar atburður á sér stað (en ekki skerðing). Í gögnunum okkar fengu 257 karlmenn hjartasjúkdóm svo þessir tímapunktar eru að hámarki 257. Við nánari athugun reynast stundum fleiri en einn karlmaður fá hjartasjúkdóm eftir jafnlangan tíma svo tímapunktarnir eru alls 248. Ef við gefum þeim heiti eftir stærðarröð þeirra, \\(t_1, \\ldots , t_{248}\\), þá væri: \\(t_1 =\\) 18, sem er sá tímapunktur þegar fyrsti maðurinn fékk hjartasjúkdóm, \\(t_2 =\\) 21 væri sá tímapunktur þegar næsti maður fékk hjartasjúkdóm \\(t_{248} =\\) 3229, væri að lokum sá tímapunktur þegar síðasti maðurinn fékk hjartasjúkdóm. Kaplan Meier metilinn er metinn á sérhverjum þessara 248 tímapunkta. Formúla metilsins er \\[ \\hat S(t) = \\Pi_{i=1}^k \\left( 1- \\frac{d_i}{n_i} \\right)\\] þar sem \\(d_i\\) táknar fjölda þeirra sem fengu atburðinn á tímapunkti \\(i\\) en \\(n_i\\) er fjöldi þeirra sem er enn í áhættu að fá atburðinn á tímapunkti \\(i\\), þ.e.a.s. hafa hvorki fengið atburðinn, né verið skertir. Í fyrsta tímapunktinum er Kaplan Meier metillinn hreinlega hlutfall þeirra sem ekki var kominn með hjartasjúkdóm af þeim sem ekki voru orðnir skertir á þeim tímapunkti (þ.e.a.s. af þeim sem voru enn í eftirfylgd). Fyrir næsta tímapunkt þarf að reikna einn mínus hlutfall fjölda þeirra sem fékk hjartasjúkdóm á þeim tímapunkti af þeim sem enn voru í eftirfylgd á þeim tímapunkti og margfalda það gildi við síðasta reiknaða Kaplan-Meier gildið. Þannig er haldið áfram koll af kolli fyrir alla tímapunktana. Skoðum nú fyrstu gildin handvirkt. Fyrir fyrsta tímapunktinn, \\(t_1\\), sem var \\(t=18\\), skoðum við hversu margir eru enn í eftirfylgd á þeim tímapunkti: sum(wcgs$time169&gt;=18) ## [1] 3154 og hversu margir þeirra höfðu ekki fengið hjartasjúkdóm á þeim tímapunkti sum(wcgs$time169&gt;=18) - sum(wcgs$time169&lt;=18&amp;wcgs$chd69==1) ## [1] 3153 Kaplan-Meier matið á tímapunkti \\(t=18\\), er því hlutfall þessara tveggja talna, eða \\(3153/3154\\) sem er 99.97 prósent. Fyrir næsta tímapunktinn, \\(t_2\\), sem var \\(t=21\\), skoðum við hversu margir eru enn í eftirfylgd á þeim tímapunkti sum(wcgs$time169&gt;=21) ## [1] 3153 og hversu margir þeirra fengu hjartasjúkdóm á þeim tímapunkti sum(wcgs$time169==21&amp;wcgs$chd69==1) ## [1] 1 Hlutfallið á tímapunkti \\(t_2\\) er því \\(1 - d_2/n_2 = 1 - 1/3153 = 3152/3153\\) Kaplan Meier gildið á tímapunkti \\(t_2\\) er þá það gildi, margfaldað með síðasta reiknaða gildi eða \\(3152/3153 \\cdot 3153/3154 = 3152/3154\\) sem er 99.94 prósent. Svona er haldið áfram koll af kolli fyrir alla 248 tímapunktana. Kaplan Meier matið er eingöngu metið á þeim tímapunktum þegar atburður á sér stað. Ef við viljum meta lifunarfallið á tímapunktum sem lenda á milli þessara gilda notum við Kaplan-Meier matið fyrir næsta tímapunkt á undan. 1.3.1 Kaplan Meier gröf Kaplan Meier gröf eru langalgengasta myndræna framsetningin á lifunargögnum. Þau eru leið til að lýsa lifunarfallinu myndrænt, þ.e.a.s. hversu hátt hlutfall þátttakanda hefur ekki fengið atburð eftir því sem tíminn líður. Gröfin teikna Kaplan Meier matið í sérhverjum tímapunkti og flatar línur milli punktanna. Gröfin eru látin “falla” á þeim tímapunktum sem atburðir eiga sér stað í samræmi við það hvernig matið er reiknað. Fyrst um sinn skulum við skoða Kaplan-Meier graf fyrir fyrstu 100 dagana. Það má sjá hér fyrir neðan. Notum tímann í árum time169y: kmfit &lt;- survfit(Surv(time169,chd69)~1,data=wcgs) km.plot &lt;- ggsurvplot(kmfit,risk.table = T,break.time.by=10,xlim=c(0,110), ylim=c(0.99,1)) km.plot Grafið byrjar í \\(y=1.00\\), þegar \\(t=0\\), enda hefur enginn fengið hjartasjúkdóm við byrjun rannsóknarinnar. Inn á grafið er ég búin að teikna punktalínur þar sem fyrsta “fallið” verður. En það er einmitt við \\(t=18\\) þegar fyrsti maðurinn fékk hjartasjúkdóm. Þar dettur línan úr 1.00 í 0.9997 eins og við vorum búin að reikna. Næsta “fall” verður við \\(t=21\\) þegar næsti maður fékk hjartaáfall og svo koll af kolli. Stækkum nú tímarammann okkar og skoðum Kaplan-Meier graf fyrir fyrstu 365 dagana: kmfit &lt;- survfit(Surv(time169,chd69)~1,data=wcgs) km.plot &lt;- ggsurvplot(kmfit,risk.table = T,break.time.by=30,xlim=c(0,365), ylim=c(0.99,1)) km.plot Núna hafa nokkur lóðrétt strik bæst við (sem eru í raun plúsar ef nána er að gáð). Þær merkingar eru þar sem skerðingar eiga sér stað, á þeim tímapunktum hefur einhver þátttakandi hætt í eftirfylgd af einhverjum ástæðum. Munið að Kaplan-Meier metilinn er bara metinn á þeim tímapunktum sem atburðir eiga sér stað, sem í þessu tilfelli eru hjartasjúkdómar, svo grafið breytist ekki á þeim tímapunkti, plúsarnir bætast bara við. Sum tímarit kjósa að fá Kaplan-Meier gröf með slíkum merkingum þegar skerðingar eru en önnur ekki. Það er ætíð hægt að stilla til í R. Skipunin survfit metur Kaplan Meier metilinn fyrir okkur. Við mötum hana með lifunarhlut til vinstri en hægra megin koma flokkarnir sem við skiptum gögnunum eftir. Til að byrja með höfum við enga skiptingu á gögnunum og setjum þess vegna bara 1 hægra megin i skipunina. kmfit &lt;- survfit(Surv(time169,chd69)~1,data=wcgs) 1.3.2 Kaplan Meier gröf eftir hópum Kaplan Meier gröf eru kjörin til að sýna mun á lifunarfallinu eftir hópum. Hér liggur beint við að skoða mun á lifun án hjartasjúkdóms eftir persónuleikagerð. Þá setjum við nafnið á þeirri breytu hægra megin í survfit skipunina þar sem 1 var áður fyrr. kmfit.2 &lt;- survfit(Surv(time169,chd69)~dibpat0f,data=wcgs) Hér breytum við skalanum í ár og skoðum tíma að hjartasjúkdómi eftir persónuleika A og B. km.plot.2 &lt;- ggsurvplot(kmfit.2,risk.table = T,xscale=365.35, break.time.by=365.25,ylim=c(0.8,1),tables.height=0.3) km.plot.2 Á þessari mynd virðist vera mikill munur á tíðni hjartasjúkdóma eftir persónuleikagerð. Blái ferillinn, sem táknar persónuleikagerð A, er allur fyrir neðan rauða ferilinn sem táknar persónuleikagerð B. Þar af leiðandi metum við að lifunarfallið sé lægra, þ.e. að líkur þess að vera án hjartasjúkdóms eru minni, sem þýðir að líkurnar á því að fá hjartasjúkdóm eru hærri. Næst munum við sjá leið til að prófa hvort sá munur sé tölfræðilega marktækur. 1.4 Miðgildi lifunar og logrank próf Algeng leið til að lýsa lifun er svokallað miðgildi lifunar (median survival). Það er sú tímalengd \\(T\\) þar sem \\(S(T)\\) verður minna en 50%. Með orðum er það sá tímapunktur þegar helmingur þátttakandanna ættu að hafa fengið atburðinn. Með því að skrifa út nafnið á lifunarhlutnum okkar fæst mat á miðgildi lifunar ásamt 95% öryggisbili. kmfit.2 ## Call: survfit(formula = Surv(time169, chd69) ~ dibpat0f, data = wcgs) ## ## n events median 0.95LCL 0.95UCL ## dibpat0f=B 1565 79 NA NA NA ## dibpat0f=A 1589 178 NA NA NA Hér vandast málið! Við sjáum bara NA. Ástæða þess er sú að karlmönnunum i WCGS rannsókninni var ekki fylgt eftir svo lengi að helmingur þeirra hefði þróað með sér hjartasjúkdóm. Í slíkum tilvikum er ekki við hæfi að lýsa lifunarfallinu með miðgildi lifunar, heldur er oft miðað við eins árs, eða fimm ára lifun, svo dæmi séu tekin. Með eins árs lifun er átt við matinu á lifunarfallinu þegar ár er liðið. Þegar lifun er mæld í dögum eins og hjá okkur er það þá \\(\\hat S(365.25)\\). Að sama skapi væri matið á fimm ára lifun \\(\\hat S(1826.25)\\). (Ath 5*365.25=18625.) Það er auðvelt að biðja R um mat á lifunarfallinu fyrir hvaða tímapunkt sem er. Það er gert með skipuninni summary() sem er mötuð með times() þar sem tímapunktarnir eru skilgreindir. summary(kmfit.2, times=c(365.25,5*365.25)) ## Call: survfit(formula = Surv(time169, chd69) ~ dibpat0f, data = wcgs) ## ## dibpat0f=B ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 365 1561 2 0.999 0.000904 0.997 1.000 ## 1826 1433 36 0.975 0.004026 0.967 0.983 ## ## dibpat0f=A ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 365 1574 13 0.992 0.00226 0.987 0.996 ## 1826 1348 88 0.934 0.00635 0.922 0.947 Við sjáum að matið á 365 daga (1 árs) lifun fyrir persónuleikagerð B er 0.99 með 95% ÖB (0.997-1.00) en matið á 365 daga lifun fyrir persónuleikagerð A er 0.992 með 95% ÖB (0.987-0.996). Í neðri línunum sjáum við matið á 1825 (5 ára) lifun. Það er 0.975 (0.967-0.983) fyrir persónuleikagerð B en 0.934 (0.922-0.947) fyrir persónuleikagerð A. Allt að ofan bendir til þess að það sé munur í tíðni hjartasjúkdóma eftir persónuleikagerðum. Ein leið til að prófa þann mun er með logrank prófi en það prófar hvort munur sé á tveimur lifunarföllum. Skipunin er einfaldlega survdiff og rithátturinn er nákvæmlega sá sami og fyrir surffit survdiff(Surv(time169,chd69)~dibpat0f,data=wcgs) ## Call: ## survdiff(formula = Surv(time169, chd69) ~ dibpat0f, data = wcgs) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## dibpat0f=B 1565 79 131 20.5 41.8 ## dibpat0f=A 1589 178 126 21.3 41.8 ## ## Chisq= 41.8 on 1 degrees of freedom, p= 1e-10 Og mikið rétt! Munurinn reyndist marktækur, p-gildið er \\(1 \\cdot 10^{-10}\\). Logrank prófið er líka hægt að nota þegar hóparnir eru fleiri en tveir og prófa það þá núlltilgátuna hvort allir ferlarnir séu eins. Núlltilgátan er að atburðir séu jafn tíðir milli hópanna. Taflan sýnir Expected eða væntanlegan fjölda miðað við þá tilgátu. En gögnin sýna (Observed) að atburðir eru færri í B hóp og fleiri í A hóp en búast mætti við ef tilgátan væri sönn. Við höfnum þeirri tilgátu og drögum þá ályktun að tíðni atburða sé meiri í A hóp. 1.5 Víti að varast og breyting á tímaskala Algeng en alvarleg mistök sem geta gerst er að ruglast á því hvort atburður eða skerðing eigi að vera táknuð með 0 eða 1. Það er þó auðvelt að bera kennsl á þvi mistök því þá stefna ferlarnir í Kaplan Meier gröfunum beinustu leið niður undir restina og enda alltaf í núlli. Hér er dæmi um slíkt. Hvað gerist ef ruglast er á atburði og skerðingu! km.fit.3 &lt;- survfit(Surv(time169,chd69==0)~dibpat0f,data=wcgs) ggsurvplot(km.fit.3) Að lokum langar okkur að sýna ykkur einfalda leið til að breyta tímaskalanum svo hann sé í árum en ekki dögum. Það er einfaldlega með því að deila í dagafjöldann með 365 þegar lifun skilgreindur. Annað hvort með því að búa til nýja breytu eins og time169y fyrir ofan eða inní survfit. km.fit.4 &lt;- survfit(Surv(time169/365.25,chd69) ~ dibpat0f,data=wcgs) ggsurvplot(km.fit.4,ylim=c(0.8,1)) "],
["some-non-parametric-procedures.html", "Chapter 2 Some Non-Parametric Procedures 2.1 2.1.2 2.2 2.1.3 Nelson - Aalen 2.3 2.2 2.4 2.2 Estimating the hazard function 2.5 An Application to Cat Adoptions", " Chapter 2 Some Non-Parametric Procedures library(tidyverse) library(survival) library(readxl) library(muhaz) library(rms) library(bshazard) Look at the methods at the following pate to estimate S(t) using the actuarial method and the Kaplan Meier method: http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Survival/BS704_Survival_print.html 2.1 2.1.2 The Kaplan Meier estimator of a survival curve is a step function, continuous from the right. At \\(t_0=0\\) we have \\(S(t_0)=1\\). Let the first survival time be \\(t_1\\) and assume \\(d_1\\) events where \\(n_1\\) were at risk. This means that \\(d_1\\) individuals had survival time equal to \\(t_1\\). Then the event probability is estimated as \\(\\hat{q}_1 = d_1/n_1\\) and the survival probability as \\(\\hat{p}_1=1-\\hat{q}_1 = 1-d_1/n_1\\). Then \\[ \\hat{S}(t_1)=(1-\\hat{q}_1)=\\left(1-\\frac{d_1}{n_1}\\right). \\] Assume \\(d_2\\) events at time \\(t_2&gt;t_1\\) where \\(n_2\\) are at risk.Since \\(S(t_2)=P[T &gt; t_2] = P[T &gt; t_2|T&gt;t_1] P[T&gt;t_1]\\), it is natural to continue with \\(\\hat{S}(t_2)= (1-\\hat{q}_2)\\hat{S}(t_1)\\). In other words: \\[ \\hat{S}(t_2)=(1-\\hat{q}_2)(1-\\hat{q}_1)=\\left(1-\\frac{d_2}{n_2}\\right)\\left(1-\\frac{d_1}{n_1}\\right). \\] Then similarly for \\(t_3\\), \\(t_4\\) etc so that for any \\(t\\) \\[ \\hat{S}(t) = \\prod_{t_i \\le t} (1-\\hat{q_î}) = \\prod_{t_i \\le t} \\left(1-\\frac{d_i}{n_i}\\right). \\] Recall that \\(P[A | B] = P[A \\cap B]/P(B)\\) and note that \\(P[T &gt; t_2]=P[T &gt; t_2 \\cap T &gt; t_1]\\), for \\(t_2&gt;t_1\\). There is only a change in \\(\\hat{S}(t)\\) when \\(t\\) is a time of an event. It does not change when \\(t\\) is a censored time. However, the number at risk changes. 2.2 2.1.3 Nelson - Aalen In an interval from \\(t_{i-1}\\) to \\(t_i\\) it is natural to estimate the hazard with the number of events in that interval divided by the person years in that interval. \\[ h(t_i) = \\frac{d_i}{n_i (t_i-t_{i-1})}. \\] The cumulative hazard can then be estimated by numerical integration: \\[ H(t) = \\sum_{t_i \\le t} \\frac{d_i}{n_i (t_i-t_{i-1})} (t_i-t_{i-1}) \\] so that \\[ H(t) = \\sum_{t_i \\le t} \\frac{d_i}{n_i}. \\] From \\(\\exp(a+b) = \\exp(a)\\exp(b)\\) we get the Nelson-Aalen estimator of \\(S(t)\\) as \\[ S(t) = \\exp(-H(t)) = \\prod_{j=1}^{k} \\exp(-d_i/n_i) \\] where \\(t_k\\) is the largest time less than \\(t\\). 2.3 2.2 We can use the status method to estimate the sampling error in \\(\\hat{S}(t)\\). The status method is used to estimate the variance of a function of a random variable. Let \\(f\\) be differentiable. It says that if \\(E(X)=\\mu\\) then \\[ \\mathrm{Var}(f(X)) \\approx \\mathrm{Var(X)}\\left(f^\\prime(\\mu)\\right)^2 \\] In particular \\[ \\mathrm{Var}(\\log(X)) \\approx \\mathrm{Var(X)}(1/\\mu)^2 \\] and \\[ \\mathrm{Var}(\\exp(X)) \\approx \\mathrm{Var(X)}(\\exp(\\mu))^2. \\] For example, \\[ \\mathrm{var}(\\log(1-\\hat{q}))=\\mathrm{var}(\\log(\\hat{p}))) \\approx \\mathrm{var}(\\hat{p})\\left(1/{p}\\right)^2=\\frac{\\hat{p}(1-\\hat{p})}{n{p}^2}, \\] which we estimate by \\[ \\frac{\\hat{p}(1-\\hat{p})}{n\\hat{p}^2} \\] Simplifying further leads to: \\[ \\mathrm{var}(\\log(1-\\hat{q})) \\approx \\frac{1-\\hat{p}}{n \\hat{p}} = \\frac{\\hat{q}}{n(1-\\hat{q})} = \\frac{d/n}{n(1-d/n)}=\\frac{d}{n(n-d)}. \\] Therefore \\[ \\mathrm{Var}(\\log(\\hat{S}(t))) = \\sum_{t_i\\le t} \\frac{d_i}{n_i(n_i-d_i)} \\] Then we can write \\(\\hat{S}(t)=\\exp(\\log(\\hat{S}(t)))\\) and use the status-method on the random variable \\(\\log(\\hat{S}(t))\\) with the function \\(f(u)=\\exp(u)\\). \\[ \\mathrm{Var}(\\hat{S}(t)) = \\mathrm{Var}(\\exp(\\log(\\hat{S}(t)))) = \\mathrm{Var}(\\log(\\hat{S}(t))) \\left(\\hat{S}(t)) \\right)^2. \\] In other words, using the results above, \\[ \\mathrm{Var}(\\hat{S}(t)) = \\left(\\hat{S}(t)) \\right)^2 \\sum_{t_i\\le t} \\frac{d_i}{n_i(n_i-d_i)}. \\] Further details can be found in the Appendix of the survival book by Hosmer, Lemeshow, and May. 2.4 2.2 Estimating the hazard function We learn a lot about the data by studying the hazard functions. But first, let’s look at the survival. We use the data from example 1.3. Survival_of_multiple_myeloma_patients &lt;- read.table(&quot;Data/Survival of multiple myeloma patients.dat&quot;, header = T) d &lt;- Survival_of_multiple_myeloma_patients %&gt;% as_tibble result.km &lt;- survfit(Surv(time, status) ~ 1,conf.type=&quot;log-log&quot;,data=d) result.na &lt;- survfit(Surv(time, status) ~ 1,conf.type=&quot;log-log&quot;,type=&quot;fh&quot;,data=d) Kaplan Meier plot(result.km) result.km ## Call: survfit(formula = Surv(time, status) ~ 1, data = d, conf.type = &quot;log-log&quot;) ## ## n events median 0.95LCL 0.95UCL ## 48 36 17 12 36 summary(result.km) ## Call: survfit(formula = Surv(time, status) ~ 1, data = d, conf.type = &quot;log-log&quot;) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 48 3 0.9375 0.0349 0.81861 0.979 ## 4 44 2 0.8949 0.0445 0.76565 0.955 ## 5 42 4 0.8097 0.0571 0.66602 0.896 ## 6 38 2 0.7670 0.0616 0.61885 0.864 ## 8 35 1 0.7451 0.0636 0.59488 0.847 ## 10 34 4 0.6575 0.0696 0.50272 0.774 ## 12 28 1 0.6340 0.0710 0.47833 0.755 ## 13 26 1 0.6096 0.0723 0.45312 0.734 ## 14 25 1 0.5852 0.0734 0.42841 0.713 ## 15 24 1 0.5608 0.0743 0.40416 0.691 ## 16 22 2 0.5098 0.0758 0.35415 0.646 ## 17 20 1 0.4844 0.0762 0.32991 0.623 ## 18 19 2 0.4334 0.0762 0.28292 0.575 ## 23 15 1 0.4045 0.0764 0.25602 0.548 ## 24 14 1 0.3756 0.0762 0.22997 0.521 ## 36 13 1 0.3467 0.0756 0.20475 0.493 ## 40 12 2 0.2889 0.0732 0.15685 0.435 ## 50 9 1 0.2568 0.0718 0.13080 0.403 ## 51 8 1 0.2247 0.0696 0.10622 0.370 ## 65 5 1 0.1798 0.0687 0.07030 0.330 ## 66 4 1 0.1348 0.0646 0.04073 0.285 ## 88 2 1 0.0674 0.0576 0.00663 0.235 ## 91 1 1 0.0000 NaN NA NA Nelson-Aalen plot(result.na) result.na ## Call: survfit(formula = Surv(time, status) ~ 1, data = d, conf.type = &quot;log-log&quot;, ## type = &quot;fh&quot;) ## ## n events median 0.95LCL 0.95UCL ## 48 36 17 12 40 summary(result.na) ## Call: survfit(formula = Surv(time, status) ~ 1, data = d, conf.type = &quot;log-log&quot;, ## type = &quot;fh&quot;) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 48 3 0.9381 0.0346 0.82037 0.980 ## 4 44 2 0.8960 0.0440 0.76792 0.955 ## 5 42 4 0.8117 0.0566 0.66921 0.897 ## 6 38 2 0.7695 0.0610 0.62246 0.865 ## 8 35 1 0.7478 0.0630 0.59871 0.848 ## 10 34 4 0.6611 0.0690 0.50737 0.777 ## 12 28 1 0.6379 0.0704 0.48322 0.757 ## 13 26 1 0.6139 0.0717 0.45828 0.737 ## 14 25 1 0.5898 0.0728 0.43382 0.716 ## 15 24 1 0.5657 0.0737 0.40982 0.695 ## 16 22 2 0.5154 0.0753 0.36034 0.650 ## 17 20 1 0.4903 0.0757 0.33635 0.627 ## 18 19 2 0.4400 0.0758 0.28978 0.580 ## 23 15 1 0.4117 0.0760 0.26325 0.554 ## 24 14 1 0.3833 0.0759 0.23751 0.527 ## 36 13 1 0.3549 0.0754 0.21256 0.500 ## 40 12 2 0.2981 0.0732 0.16504 0.444 ## 50 9 1 0.2668 0.0719 0.13931 0.412 ## 51 8 1 0.2354 0.0700 0.11492 0.380 ## 65 5 1 0.1928 0.0691 0.08031 0.341 ## 66 4 1 0.1501 0.0656 0.05087 0.299 ## 88 2 1 0.0911 0.0605 0.01617 0.249 ## 91 1 1 0.0335 0.0402 0.00113 0.183 In R we have the muhaz and pehaz functions to estimate the hazard. attach(d) result.pe12 &lt;- pehaz(time, status, width=12) ## ## max.time= 91 ## width= 12 ## nbins= 8 result.smooth &lt;- muhaz(time, status) detach(d) The output from pehaz: result.pe12 ## ## Call: ## pehaz(times = time, delta = status, width = 12) ## ## Bin Width: ## [1] 12 ## ## Cuts Defining the Bins: ## [1] 0 12 24 36 48 60 72 84 96 ## ## Hazard Estimate for Each Bin: ## [1] 0.034934498 0.044444444 0.006410256 0.025000000 0.025974026 0.042553191 ## [7] 0.000000000 0.181818182 ## ## Number of Events in Each Bin: ## [1] 16 10 1 3 2 2 0 2 ## ## Number at Risk in Each Bin: ## [1] 48 28 14 13 9 5 3 2 ## ## Total Follow-up Time in Each Bin: ## [1] 458 225 156 120 77 47 28 11 The hazard in interval \\(i\\) is estimated with \\(d_i / \\mathrm{Follow-up}_i\\) where \\(\\mathrm{Follow-up}\\) is the Total Follow-up Time \\(i\\). If we take 12 months as a fixed time interval, we get on one hand from \\(d_i/n_i\\) result.pe12$Events/(result.pe12$At.Risk*12) ## [1] 0.027777778 0.029761905 0.005952381 0.019230769 0.018518519 0.033333333 ## [7] 0.000000000 0.083333333 and on the other hand the correct result. result.pe12$Hazard ## [1] 0.034934498 0.044444444 0.006410256 0.025000000 0.025974026 0.042553191 ## [7] 0.000000000 0.181818182 result.pe12$Events/result.pe12$F.U.Time ## [1] 0.034934498 0.044444444 0.006410256 0.025000000 0.025974026 0.042553191 ## [7] 0.000000000 0.181818182 Note that using number at risk times 12 leads to overestimate in the person-years and therefore underestimate in the hazard. This difference gets smaller with smaller intervals. Graphical comparison attach(d) result.pe6 &lt;- pehaz(time, status, width=6) ## ## max.time= 91 ## width= 6 ## nbins= 16 detach(d) plot(result.pe6,xlim=c(0,80),ylim=c(0,0.1)) lines(result.pe12) points(result.pe6$Cuts[-1],result.pe6$Hazard) lines(result.smooth$est.grid,result.smooth$haz.est,col=&quot;blue&quot;) This is a comparison of the Nelson Aalen estimator (line) with the result from pehaz. Note \\(H(t)=-\\log(S)\\) is on the y-axis. plot(result.na$time,-log(result.na$surv)) lines(result.pe6$Cuts[-1],cumsum(result.pe6$Hazard)*6) Comparison using the Kaplan-Meier results and transformations based pehaz (blue line). plot(result.km,xlim=c(0,90)) lines(result.pe6$Cuts[-1],exp(-cumsum(result.pe6$Hazard)*6),col=&quot;blue&quot;) Results from the rms package rms1 &lt;- npsurv(Surv(time, status) ~ 1,data=d) survplot(rms1,what=c(&quot;survival&quot;),n.risk = T) 2.5 An Application to Cat Adoptions 2.5.1 Data We’ll work with the AustinCats data from the rethinking package, the companion package to the book Statistical Rethinking by Richard McElreath. data(&quot;AustinCats&quot;, package = &quot;rethinking&quot;) d &lt;- AustinCats %&gt;% as_tibble %&gt;% filter(out_event %in% c(&quot;Adoption&quot;, &quot;Censored&quot;)) %&gt;% select(days_to_event, out_event) rm(AustinCats) 2.5.2 Kaplan-Meier In the Kaplan-Meier estimate, the survivor function is estimated and then transformed to the hazard function. km &lt;- d %&gt;% mutate(n = n()) %&gt;% group_by(days_to_event, n) %&gt;% summarise(events = sum(out_event == &quot;Adoption&quot;), censored = n() - events, lost = events + censored) %&gt;% ungroup %&gt;% mutate(total_lost = cumsum(lost), n = n - lag(total_lost, default = 0), conditional_survival = (n - events) / n, surv = cumprod(conditional_survival), cumulative_hazard = -log(surv), hazard = cumulative_hazard - lag(cumulative_hazard, default = 0)) %&gt;% rename(Hazard = hazard, &quot;Cumulative Hazard&quot; = cumulative_hazard, &quot;Survival&quot; = surv) %&gt;% pivot_longer(c(Hazard, &quot;Cumulative Hazard&quot;, Survival)) %&gt;% mutate(type = &quot;Kaplan-Meier&quot;) km %&gt;% ggplot(aes(days_to_event, value)) + geom_step() + facet_wrap(&quot;name&quot;, scales = &quot;free&quot;) + labs(x = &quot;Time (days)&quot;, title = &quot;Kaplan-Meier estimates for cat adoptions&quot;) + theme(axis.title.y = element_blank()) 2.5.3 Nelson-Aalen In the Nelson-Aalen estimate, the hazard function is estimated and then transformed to the survivor function. na &lt;- d %&gt;% mutate(n = n()) %&gt;% group_by(days_to_event, n) %&gt;% summarise(events = sum(out_event == &quot;Adoption&quot;), censored = n() - events, lost = events + censored) %&gt;% ungroup %&gt;% mutate(total_lost = cumsum(lost), n = n - lag(total_lost, default = 0), hazard = events / n, cumulative_hazard = cumsum(hazard), surv = exp(-cumulative_hazard)) %&gt;% rename(Hazard = hazard, &quot;Cumulative Hazard&quot; = cumulative_hazard, &quot;Survival&quot; = surv) %&gt;% pivot_longer(c(Hazard, &quot;Cumulative Hazard&quot;, Survival)) %&gt;% mutate(type = &quot;Nelson-Aalen&quot;) na %&gt;% ggplot(aes(days_to_event, value)) + geom_step() + facet_wrap(&quot;name&quot;, scales = &quot;free&quot;) + labs(x = &quot;Time (days)&quot;, title = &quot;Nelson-Aalen estimates for cat adoptions&quot;) + theme(axis.title.y = element_blank()) 2.5.4 Compare km %&gt;% bind_rows(na) %&gt;% ggplot(aes(days_to_event, value)) + geom_step() + facet_grid(name ~ type, scales = &quot;free&quot;) + labs(x = &quot;Time (days)&quot;, title = &quot;Comparison of Kaplan-Meier and Nelson-Aalen estimates&quot;, subtitle = &quot;Based on cat adoption data from Statistical Rethinking by Richard McElreath&quot;) + theme(axis.title.y = element_blank()) "],
["the-cox-regression-model.html", "Chapter 3 The Cox Regression Model 3.1 Inngangur 3.2 Modeling the hazard function 3.3 Fitting the Cox regression model", " Chapter 3 The Cox Regression Model library(survival) library(tidyverse) library(flexsurv) library(rms) library(skimr) 3.1 Inngangur Þessi kafli fjallar um Cox líkanið sem er aðhvarfsgreiningarlíkan. Við viljum færa okkur yfir í afhvarfsgreiningu til að meta áhrifsstærðir og til að geta leiðrétt fyrir skýribreytum (explanatory variables). Aðhvarfsgreining hefur þá möguleika umfram einföld próf til að bera saman hópa. Log-rank prófið segir okkur bara að munur sé á lifun milli hópa en ekki hve mikill hann er. Hægt er að leiðrétta með því að gera stratified próf. En ekki hægt að leiðrétta fyrir samfelldum breytum. 3.2 Modeling the hazard function Markmiðið er að búa til líkan fyrir hættuföllin þ.e. hazard föllin og lýsa hvernig bakgrunnsreytur hafa áhrif á þau. Tökum dæmi úr sýnidæmi 1.3 sem fjallar um lifun sjúklinga með multiple myeloma Survival_of_multiple_myeloma_patients &lt;- read.table(&quot;Data/Survival of multiple myeloma patients.dat&quot;, header=T) d_1_3 &lt;- Survival_of_multiple_myeloma_patients Skýribreyturnar þar eru: names(d_1_3)[-c(1:3)] ## [1] &quot;age&quot; &quot;sex&quot; &quot;bun&quot; &quot;ca&quot; &quot;hb&quot; &quot;pcells&quot; &quot;protein&quot; Fyrstu línur eru: head(d_1_3) ## patient time status age sex bun ca hb pcells protein ## 1 1 13 1 66 1 25 10 14.6 18 1 ## 2 2 52 0 66 1 13 11 12.0 100 0 ## 3 3 6 1 53 2 15 13 11.4 33 1 ## 4 4 40 1 69 1 10 10 10.2 30 1 ## 5 5 10 1 65 1 20 10 13.2 66 0 ## 6 6 7 0 57 2 12 8 9.9 45 0 Í sýnidæmi 1.4 er borin saman lifun sjúklinga með blöðruhálskirtilskrabbamein eftir meðferð. Comparison_of_two_treatments_for_prostatic_cancer &lt;- read.table(&quot;Data/Comparison of two treatments for prostatic cancer.dat&quot;,header=T) d_1_4 &lt;- Comparison_of_two_treatments_for_prostatic_cancer d_1_4 &lt;- d_1_4 %&gt;% mutate(treatmentf=factor(treatment)) Meðferðin er treatment dálkurinn og fyrstu línur eru: head(d_1_4) ## patient treatment time status age shb size index treatmentf ## 1 1 1 65 0 67 13.4 34 8 1 ## 2 2 2 61 0 60 14.6 4 10 2 ## 3 3 2 60 0 77 15.6 3 8 2 ## 4 4 1 58 0 64 16.2 6 9 1 ## 5 5 2 51 0 65 14.1 21 9 2 ## 6 6 1 51 0 61 13.5 8 8 1 3.2.1 A model for comparison of two groups Byrjum á að bera saman tvo hópa og notum gögnin úr sýnidæmi 1.4. Meðferðirnar eru tvær: table(d_1_4$treatmentf) ## ## 1 2 ## 18 20 Við eigum alltaf að byrja á að meta fjölda atburða, eftirfylgnitíma og tíðni atburða á tímaeiningu. r0 &lt;- d_1_4 %&gt;% summarise(n=n(),d=sum(status), V=sum(time),lambda=d/V,se.lambda=sqrt(d)/V) r0 ## n d V lambda se.lambda ## 1 38 6 1890 0.003174603 0.001296026 r &lt;- d_1_4 %&gt;% group_by(treatmentf) %&gt;% summarise(n=n(), d=sum(status), V=sum(time),lambda=d/V,se.lambda=sqrt(d)/V) r ## # A tibble: 2 x 6 ## treatmentf n d V lambda se.lambda ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 18 5 821 0.00609 0.00272 ## 2 2 20 1 1069 0.000935 0.000935 Við sjáum að tíðni atburða er lægri fyrir meðferð tvö og hættuhlutfallið er exp(diff(log(r$lambda))) ## [1] 0.1536015 Reyndar er þetta mat það sama og ef við miðum við veldisdreifingu á lifunartímanum. Einfalt er að meta hættuhlutfallið með flexurvreg fallinu í flexurv pakkanum: f0 &lt;- flexsurvreg(Surv(time,status) ~ 1,data=d_1_4,dist=&quot;exponential&quot;) f0 ## Call: ## flexsurvreg(formula = Surv(time, status) ~ 1, data = d_1_4, dist = &quot;exponential&quot;) ## ## Estimates: ## est L95% U95% se ## rate 0.00317 0.00143 0.00707 0.00130 ## ## N = 38, Events: 6, Censored: 32 ## Total time at risk: 1890 ## Log-likelihood = -40.51544, df = 1 ## AIC = 83.03087 f1 &lt;- flexsurvreg(Surv(time,status) ~ treatmentf,data=d_1_4,dist=&quot;exponential&quot;) f1 ## Call: ## flexsurvreg(formula = Surv(time, status) ~ treatmentf, data = d_1_4, ## dist = &quot;exponential&quot;) ## ## Estimates: ## data mean est L95% U95% se exp(est) ## rate NA 0.00609 0.00253 0.01463 0.00272 NA ## treatmentf2 0.52632 -1.87339 -4.02043 0.27364 1.09545 0.15360 ## L95% U95% ## rate NA NA ## treatmentf2 0.01795 1.31474 ## ## N = 38, Events: 6, Censored: 32 ## Total time at risk: 1890 ## Log-likelihood = -38.4799, df = 2 ## AIC = 80.95981 Í fyrsta líkaninu er engin skýribreyta og fæst þá tíðnin fyrir allan hópinn. Berið saman við r0 að ofan. Í seinna líkaninu býr flexurvreg til dummy breytu þannig að ef treatment er 1 fær dummy breytan gildið 0 en ef treatment er 2 fær dummy breytan gildið 1. Grunnhættufallið sem flexurvreg metur er fyrir öll gildi á skýribreytum sem 0. Hér er bara ein skýribreyta og þegar hún er 0 gildið að treatment er 1. Að ofan gefur línan rate þá matið á lambda í veldisdreifingunni þegar treatment er 1. Síðan gefur línan treatmentf2 mismuninn á log-rate á treatment=2 miðað við treatment=1. beta &lt;- diff(log(r$lambda)) beta ## [1] -1.873394 Hættuhlutfallið er exp(beta) ## [1] 0.1536015 Öryggisbilið er vítt og inniheldur 1. Það næst ekki að sýna fram á tölfræðilega marktækan mun hér með Wald prófinu. exp(confint(f1))[2,] ## 2.5 % 97.5 % ## 0.01794531 1.31474020 Log-likelihood prófið er aflmeira og rétt nær að sýna fram á martkækan mun ll &lt;- -2*(logLik(f0)-logLik(f1)) ll ## &#39;log Lik.&#39; 4.071062 (df=1) 1-pchisq(ll,1) ## &#39;log Lik.&#39; 0.04362384 (df=1) Athugið að hættuhlutfallið er um 85% lægra í treatment=1 miðað við treatment=2! Það væri til mikils að rannsaka þetta betur með stærra þýði. Hér getum við skrifað \\(h_0(t) = \\lambda\\) fyrir treatment=1 og \\(h_1(t) = \\exp(\\beta) h_0(t)\\) fyrir treatment=2. Þetta er einfaldasta proportional hazards líkanið. Við getum valið annað form á \\(h_0(t)\\) (t.d. Weibull) eða haft óstikað mat á \\(h_0(t)\\) og erum við þá komin með líkan Cox. Við höfum þegar kynnst alveg óstikuðu mati á lifunartíma. Við skulum rifja það upp og teikna stikaða matið með óstikaða matinu úr aðferð Kaplan-Meier. Við höfum að \\(h_0(t) = \\lambda\\). Þá er \\(H_0(t) = \\lambda t\\) og \\(S_0(t) = \\exp(-\\lambda t)\\). Svo er \\(h_1(t) = \\exp(\\beta) h_0(t)\\) og þá \\(H_1(t) = \\exp(\\beta) \\lambda t\\) og \\(S_1(t) =\\exp(-\\exp(\\beta) \\lambda t) = \\exp(-\\lambda t) ^ {\\exp(\\beta)} = S_0(t) ^{\\exp(\\beta)}\\). s1 &lt;- survfit(Surv(time,status) ~ treatmentf,data=d_1_4) plot(s1) t &lt;- 0:70 lines(t,exp(-0.00609 * t),col=&quot;blue&quot;) lines(t,exp(-0.00609 * t) ^ exp( -1.87339 ),col=&quot;red&quot;) Óstikað próf á mun má milli meðferða væri gert með log rank: survdiff(Surv(time,status) ~ treatmentf,data=d_1_4) ## Call: ## survdiff(formula = Surv(time, status) ~ treatmentf, data = d_1_4) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## treatmentf=1 18 5 2.47 2.58 4.42 ## treatmentf=2 20 1 3.53 1.81 4.42 ## ## Chisq= 4.4 on 1 degrees of freedom, p= 0.04 Hér fæst marktækt lægri tíðni atburða í treatment 2 miðað við treatment 1. Til að undirbúa okkur fyrir Cox líkanið skulum við skoða niðurstöðuna úr því líkani með coxph. Hér er \\(h_0(t)\\) óstikað og við fáum bara hlutfallið \\(\\exp(\\beta)\\) á milli \\(h_1(t)\\) og \\(h_0(t)\\). coxph(Surv(time,status) ~ treatmentf,data=d_1_4,ties = &quot;breslow&quot;) ## Call: ## coxph(formula = Surv(time, status) ~ treatmentf, data = d_1_4, ## ties = &quot;breslow&quot;) ## ## coef exp(coef) se(coef) z p ## treatmentf2 -1.9780 0.1384 1.0982 -1.801 0.0717 ## ## Likelihood ratio test=4.55 on 1 df, p=0.03293 ## n= 38, number of events= 6 Niðurstöðurnar eru líkar niðurstöðunni að ofan úr veldisdreifingunni, en ekki alveg eins. Við lærum seinna hvernig við náum í matið á \\(h_0(t)\\) úr Cox líkaninu. 3.2.2 The general proportional hazards model Almennt skrifum við proportional hazards líkanið svona \\[ h(t) = h_0(t) \\exp(\\beta_1 x_1 + \\cdots + \\beta_p x_p). \\] Með því að deila með \\(h_0(t)\\) og taka logra fæst: \\[ \\log \\left( \\frac{h(t)}{h_0(t)} \\right) = \\beta_1 x_1 + \\cdots + \\beta_p x_p. \\] Með öðrum orðum er logrinn af áhættuhlutfallinu línuleg samantekt af skýribreytunum. Farið er með skýribreytur alveg eins í öðrum línulegum líkönum eins og aðhvarfsgreiningu og tvíkosta aðhvarfsgreiningu. 3.3 Fitting the Cox regression model Hér er stuttlega sýnt hvernig eitt hættuhlutfall er fengið með Cox líkaninu. Prognosis_for_women_with_breast_cancer &lt;- read.table(&quot;Data/Prognosis for women with breast cancer.dat&quot;, header=T) d_1_2 &lt;- Prognosis_for_women_with_breast_cancer d_1_2 &lt;- d_1_2 %&gt;% mutate(stainf = factor(stain)) En fyrst alltaf að skoða tíðni atburða: r &lt;- d_1_2 %&gt;% group_by(stainf) %&gt;% summarise(n=n(), d=sum(status), V=sum(time),lambda=d/V,se.lambda=sqrt(d)/V) r ## # A tibble: 2 x 6 ## stainf n d V lambda se.lambda ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 13 5 1652 0.00303 0.00135 ## 2 2 32 21 2679 0.00784 0.00171 r$lambda ## [1] 0.003026634 0.007838746 diff(log(r$lambda)) ## [1] 0.9516276 exp(diff(log(r$lambda))) ## [1] 2.589922 Svo Cox líkanið: f2 &lt;- coxph(Surv(time, status) ~ stainf,data=d_1_2,ties = &quot;breslow&quot;) f2 ## Call: ## coxph(formula = Surv(time, status) ~ stainf, data = d_1_2, ties = &quot;breslow&quot;) ## ## coef exp(coef) se(coef) z p ## stainf2 0.9080 2.4794 0.5009 1.813 0.0699 ## ## Likelihood ratio test=3.87 on 1 df, p=0.04911 ## n= 45, number of events= 26 exp(coef(f2)) ## stainf2 ## 2.479398 exp(confint(f2)) ## 2.5 % 97.5 % ## stainf2 0.9288808 6.618086 Næst skoðum við fjölbreytulíkan úr multiple myaloma gögnunum. skim(d_1_3) Table 3.1: Data summary Name d_1_3 Number of rows 48 Number of columns 10 _______________________ Column type frequency: numeric 10 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist patient 0 1 24.50 14.00 1.0 12.75 24.5 36.25 48.0 ▇▇▇▇▇ time 0 1 23.38 23.72 1.0 6.75 14.5 37.00 91.0 ▇▁▂▁▁ status 0 1 0.75 0.44 0.0 0.75 1.0 1.00 1.0 ▂▁▁▁▇ age 0 1 62.90 6.96 50.0 58.75 62.5 68.25 77.0 ▅▇▇▇▃ sex 0 1 1.40 0.49 1.0 1.00 1.0 2.00 2.0 ▇▁▁▁▅ bun 0 1 33.92 35.91 6.0 13.75 21.0 39.25 172.0 ▇▂▁▁▁ ca 0 1 9.94 1.45 8.0 9.00 10.0 10.00 15.0 ▇▆▃▁▁ hb 0 1 10.25 2.79 4.9 8.65 10.2 12.57 14.6 ▃▂▇▅▆ pcells 0 1 42.94 30.02 3.0 21.25 33.0 63.00 100.0 ▇▇▆▃▅ protein 0 1 0.31 0.47 0.0 0.00 0.0 1.00 1.0 ▇▁▁▁▃ Hér setjum við allar breytur inn. En hugsanlea mundum við laga þær til. Breyta í factor og miðja en það kemur síðar: f4 &lt;- coxph(Surv(time,status) ~ age + sex + bun + ca + hb + pcells + protein,data = d_1_3,ties=&quot;breslow&quot; ) f4 ## Call: ## coxph(formula = Surv(time, status) ~ age + sex + bun + ca + hb + ## pcells + protein, data = d_1_3, ties = &quot;breslow&quot;) ## ## coef exp(coef) se(coef) z p ## age -0.019358 0.980828 0.027924 -0.693 0.488159 ## sex -0.250899 0.778101 0.402286 -0.624 0.532836 ## bun 0.020826 1.021044 0.005929 3.513 0.000443 ## ca 0.013125 1.013211 0.132442 0.099 0.921061 ## hb -0.135241 0.873506 0.068891 -1.963 0.049635 ## pcells -0.001594 0.998407 0.006577 -0.242 0.808533 ## protein -0.640438 0.527061 0.426687 -1.501 0.133367 ## ## Likelihood ratio test=16.24 on 7 df, p=0.02302 ## n= 48, number of events= 36 Frank Harrell er líka með fall fyrir Cox likanið. Það gefur meiri upplýsingar t.d. R2 og Dxy sem er skylt concordance index C þannig að \\(C = 0.5(Dxy+1)\\) f5 &lt;- cph(Surv(time,status) ~ age + sex + bun + ca + hb + pcells + protein,data = d_1_3,ties=&quot;breslow&quot; ) f5 ## Cox Proportional Hazards Model ## ## cph(formula = Surv(time, status) ~ age + sex + bun + ca + hb + ## pcells + protein, data = d_1_3, ties = &quot;breslow&quot;) ## ## Model Tests Discrimination ## Indexes ## Obs 48 LR chi2 17.53 R2 0.309 ## Events 36 d.f. 7 Dxy 0.410 ## Center -2.219 Pr(&gt; chi2) 0.0143 g 0.940 ## Score chi2 25.59 gr 2.561 ## Pr(&gt; chi2) 0.0006 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## age -0.0181 0.0278 -0.65 0.5165 ## sex -0.2495 0.4031 -0.62 0.5360 ## bun 0.0227 0.0061 3.71 0.0002 ## ca 0.0133 0.1327 0.10 0.9204 ## hb -0.1330 0.0685 -1.94 0.0523 ## pcells -0.0014 0.0066 -0.21 0.8366 ## protein -0.6833 0.4294 -1.59 0.1116 ## Svo er mjög gagnlegt að bera saman spágetu breytanna út frá kí-kvaðrat gildum og bera saman. Hér sést í fljótu bragði að bun, hb og e.t.v. protein skipta mestu máli. plot(anova(f5)) "],
["model-checking-in-the-cox-regression-model.html", "Chapter 4 Model Checking in the Cox Regression Model 4.1 Einfalt Kaplan Meier 4.2 Veldislifunarfallið 4.3 Leifar í Cox 4.4 Survminer", " Chapter 4 Model Checking in the Cox Regression Model library(survival) library(tidyverse) library(flexsurv) library(rms) library(skimr) library(stdReg) library(survminer) Survival_of_multiple_myeloma_patients &lt;- read.table(&quot;./data/Survival of multiple myeloma patients.dat&quot;, header=T) d_1_3 &lt;- Survival_of_multiple_myeloma_patients d_1_3$sexf &lt;- factor(d_1_3$sex) d_1_3$proteinf &lt;- factor(d_1_3$protein) head(d_1_3) ## patient time status age sex bun ca hb pcells protein sexf proteinf ## 1 1 13 1 66 1 25 10 14.6 18 1 1 1 ## 2 2 52 0 66 1 13 11 12.0 100 0 1 0 ## 3 3 6 1 53 2 15 13 11.4 33 1 2 1 ## 4 4 40 1 69 1 10 10 10.2 30 1 1 1 ## 5 5 10 1 65 1 20 10 13.2 66 0 1 0 ## 6 6 7 0 57 2 12 8 9.9 45 0 2 0 4.1 Einfalt Kaplan Meier m1 &lt;- survfit(Surv(time, status) ~ 1, data = d_1_3) plot(m1) total_time &lt;- sum(d_1_3$time) num_events &lt;- sum(d_1_3$status) lambda &lt;- num_events / total_time lambda ## [1] 0.03208556 Tími á milli atburða er 1 / lambda 1 / lambda ## [1] 31.16667 4.2 Veldislifunarfallið s &lt;- function(t) exp(-lambda * t) s(20) ## [1] 0.5263909 t &lt;- seq(0, 90) plot(t, s(t), type = &quot;l&quot;) plot(m1) lines(x = t, y = s(t), col = &quot;blue&quot;) \\[ S(t) = e^{-\\lambda t} \\] \\[ H(t) = -\\log S(t) = -\\log (e^{-\\lambda t}) = -(-\\lambda t) = \\lambda t \\] \\[ H(t) = \\int_0^t h(t)dt = \\sum_{i=1}^n h(t)(t_{i + 1} - t_i) \\] plot(m1, fun = &quot;cumhaz&quot;) lines(t, lambda * t, col = &quot;blue&quot;) 4.3 Leifar í Cox head(d_1_3) ## patient time status age sex bun ca hb pcells protein sexf proteinf ## 1 1 13 1 66 1 25 10 14.6 18 1 1 1 ## 2 2 52 0 66 1 13 11 12.0 100 0 1 0 ## 3 3 6 1 53 2 15 13 11.4 33 1 2 1 ## 4 4 40 1 69 1 10 10 10.2 30 1 1 1 ## 5 5 10 1 65 1 20 10 13.2 66 0 1 0 ## 6 6 7 0 57 2 12 8 9.9 45 0 2 0 m_4_5 &lt;- coxph(Surv(time, status) ~ hb + bun, data = d_1_3) m_4_5 ## Call: ## coxph(formula = Surv(time, status) ~ hb + bun, data = d_1_3) ## ## coef exp(coef) se(coef) z p ## hb -0.134952 0.873758 0.061956 -2.178 0.029391 ## bun 0.020043 1.020245 0.005816 3.446 0.000569 ## ## Likelihood ratio test=13.98 on 2 df, p=0.0009213 ## n= 48, number of events= 36 Cox-Snell residuals eiga að hegða sér eins og lifunargögn úr veldisdreifingu með \\(\\lambda = 1\\). resid_martingale &lt;- residuals(m_4_5, type = &quot;martingale&quot;) d_1_3$coxsnell &lt;- d_1_3$status - resid_martingale m_coxsnell &lt;- survfit(Surv(coxsnell, status) ~ 1, data = d_1_3) total_coxsnell &lt;- sum(d_1_3$coxsnell) lambda_coxsnell &lt;- num_events / total_coxsnell lambda_coxsnell ## [1] 1 plot(m_coxsnell, fun = &quot;cumhaz&quot;) abline(a = 0, b = 1, col = &quot;blue&quot;) 4.4 Survminer ggcoxdiagnostics(m_4_5) ggcoxfunctional(m_4_5) m_null &lt;- coxph(Surv(time, status) ~ 1, data = d_1_3) resid_martin &lt;- residuals(m_null, type = &quot;martingale&quot;) d_1_3$resid_martin &lt;- resid_martin ggplot(data = d_1_3, aes(bun, resid_martin)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + scale_x_log10() d_1_3 %&gt;% mutate(log_bun = log(bun)) %&gt;% gather(variable, value, log_bun, bun, hb) %&gt;% ggplot(aes(value, resid_martin)) + geom_point() + geom_smooth() + facet_wrap(&quot;variable&quot;, scales = &quot;free&quot;) 4.4.1 Deviance residuals ggcoxdiagnostics(m_4_5, type = &quot;deviance&quot;) 4.4.2 dfbeta ggcoxdiagnostics(m_4_5, type = &quot;dfbeta&quot;) "],
["parametric-proportional-hazards-models.html", "Chapter 5 Parametric Proportional Hazards Models 5.1 Introduction 5.2 Exponential survival function for the survival time 5.3 The Weibull survival function 5.4 Estimating the hazard function and survival 5.5 Covariates and Hazard ratios 5.6 Test of interaction using flexurvreg 5.7 Comparison with a Cox model 5.8 Estimating risk", " Chapter 5 Parametric Proportional Hazards Models library(muhaz) library(flexsurv) library(eha) library(mice) # for Nelson Aalen library(epitools) library(rms) library(bshazard) knitr::include_app(&quot;https://bgautijonsson.shinyapps.io/ParametricModels/&quot;, height = &quot;800px&quot;) data(&quot;wcgs&quot;) wcgs$age0q5 &lt;- cut(wcgs$age0,breaks=quantile(wcgs$age0,seq(0,1,0.2)),include.lowest=T,right=FALSE) wcgs$behpat0f &lt;- factor(wcgs$behpat0,levels=1:4,label=c(&quot;A1&quot;,&quot;A2&quot;,&quot;B1&quot;,&quot;B2&quot;)) wcgs$dibpat0f &lt;- factor(wcgs$dibpat0,levels=0:1,label=c(&quot;B&quot;,&quot;A&quot;)) wcgs$smoker &lt;- ifelse(wcgs$ncigs0&gt;0,1,0) wcgs$smokerf &lt;- factor(wcgs$smoker,levels=c(1,0),labels=c(&quot;Yes&quot;,&quot;No&quot;)) wcgs$smokerfny &lt;- factor(wcgs$smoker,levels=c(0,1),labels=c(&quot;No&quot;,&quot;Yes&quot;)) wcgs$heightcm &lt;- wcgs$height0*2.54 wcgs$weightkg &lt;- wcgs$weight0*0.45359237 wcgs$bmi &lt;- wcgs$weightkg / (wcgs$heightcm/100)^2 wcgs$cholmmol = wcgs$chol/39 wcgs$chd69f &lt;- factor(wcgs$chd69,levels=c(1,0),labels=c(&quot;Yes&quot;,&quot;No&quot;)) wcgs$chd69fny &lt;- factor(wcgs$chd69,levels=c(0,1),labels=c(&quot;No CHD&quot;,&quot;CHD&quot;)) wcgs$bmiq5 &lt;- cut(wcgs$bmi,breaks=quantile(wcgs$bmi,seq(0,1,0.2)),include.lowest=T,right=FALSE) wcgs$time169y &lt;- wcgs$time169/365.24 # restricted followup to 5 years wcgs$time169y5 &lt;- pmin(wcgs$time169y,5) wcgs$chd695 &lt;- (wcgs$chd69==1 &amp; wcgs$time169y &lt;=5) wcgs$agec &lt;- wcgs$age0 - 46 wcgs$cholmmolc &lt;- wcgs$cholmmol - 5.8 wcgs$sbpc &lt;- wcgs$sbp0 - 128 5.1 Introduction We continue to use the wcgs data included with the epitools package. WCGS stands for the Western Collaborative Group Study. If the rate of events follow a Poisson distribution it can be shown that the time between events, or the time until next event, follow an exponential distribution. Overall incidence or hazard rate in the WCGS data was fut &lt;- sum(wcgs$time169y) en &lt;- sum(wcgs$chd69) fut ## [1] 23176.25 en ## [1] 257 en/fut ## [1] 0.01108894 For every 1000 person years: en/fut*1000 ## [1] 11.08894 The incidence can also be estimated from the Poisson model using an offset. The term offset means that we standardize the number of outcomes by the time. We use log of time because the Poisson model has a log-link. fit0 &lt;- glm(chd69 ~ 1 + offset(log(time169y)),family=poisson(),data=wcgs) exp(coef(fit0))*1000 ## (Intercept) ## 11.08894 exp(confint.default(fit0))*1000 ## 2.5 % 97.5 % ## (Intercept) 9.812822 12.53101 The cumulative incidence is then simply \\(\\lambda t\\) after observation length \\(T=t\\). This is an assumption that needs further inspection. 5.2 Exponential survival function for the survival time We’ll begin by plotting the hazard function, which is the rate of events over a small time interval. Given that time \\(t\\) has passed, the probability an event will take place at time \\(T\\) in the interval between \\(t\\) and \\(t+\\delta\\) is P[t T &lt; t + |T t]. The hazard rate is this probability standardized by a small time interval, and then we get a rate when the interval goes to zero length: \\[ h(t) = \\frac{P[t \\le T &lt; t + \\delta|T \\ge t]}{\\delta}. \\] Note that \\(h() \\ge 0\\). The connection between the exponential distribution and the Poisson model is that if the Poisson model holds then the exponential model holds for the time and the hazard is constant: \\[ h(t) = \\lambda \\] We’ll estimate the hazard using the muhaz function in R. The rate from the Poisson model, assuming constant hazard, is superimposed. fitm &lt;- bshazard(Surv(time169y,chd69)~1,data=wcgs) ## Iterations: relative error in phi-hat = 1e-04 ## phi= 1.400538 sv2= 0.03443796 df= 10.75369 lambda= 40.66844 ## phi= 1.428413 sv2= 0.008229519 df= 7.815714 lambda= 173.5718 ## phi= 1.462473 sv2= 0.002951656 df= 5.667443 lambda= 495.4755 ## phi= 1.493406 sv2= 0.001583692 df= 4.551427 lambda= 942.9903 ## phi= 1.514479 sv2= 0.001057151 df= 4.007546 lambda= 1432.604 ## phi= 1.52876 sv2= 0.0007761689 df= 3.701527 lambda= 1969.622 ## phi= 1.539557 sv2= 0.00059308 df= 3.490586 lambda= 2595.868 ## phi= 1.548565 sv2= 0.0004582588 df= 3.321545 lambda= 3379.238 ## phi= 1.556631 sv2= 0.0003512205 df= 3.171027 lambda= 4432.063 ## phi= 1.564196 sv2= 0.0002626861 df= 3.026499 lambda= 5954.622 ## phi= 1.571455 sv2= 0.0001886837 df= 2.88026 lambda= 8328.516 ## phi= 1.578402 sv2= 0.0001280916 df= 2.727957 lambda= 12322.45 haz.frame &lt;- data.frame(x=fitm$time,y=fitm$hazard) ggplot(haz.frame,aes(x,y))+geom_line()+geom_hline(yintercept = exp(coef(fit0)),lty=2)+ xlab(&quot;t (years)&quot;) + ylab(&quot;h(t)&quot;) It seems that the rate is increasing along with the duration of the follow up. The cohort is getting older as time passes and there is no renewal of younger men into the cohort. The survival function for an exponential distribution is \\[ S(t) = P[T&gt;t] = \\exp(-\\lambda t). \\] The cumulative hazard function \\(H\\) is the integral of the hazard function or \\[ H(t) = \\int_0^t h(u) du = \\int_0^t \\lambda \\, du = \\lambda t. \\] Note that a general result from survival analysis says that \\[ S(t) = \\exp(-H(t)) \\] The flexsurv package can be used to get an estimate for \\(\\lambda\\) for the exponential distribution. Note that there are two different ways to present the exponential and the Weibull distributions in survival analysis. Either using the proportional hazards (PH) definition or the accelerated failure time (AFT) definition. We’ll consider the PH approach. It is possible to compute estimates from one approach to the other. Recall the proportional hazards model: \\[ h(t) = h_0(t) \\exp(\\beta_1 x_1 + \\cdots + \\beta_p x_p) \\] Equivalently \\[ \\log h(t) = \\log h_0(t) + \\beta_1 x_1 + \\cdots + \\beta_p x_p \\] Here \\(x_1,\\ldots,x_p\\) are covariates. Applying flexsurvreg, assuming the exponential distribution, with no covariates gives fit.e.ph &lt;- flexsurvreg(Surv(time169y,chd69)~1,dist=&quot;exponential&quot;, data=wcgs) fit.e.ph ## Call: ## flexsurvreg(formula = Surv(time169y, chd69) ~ 1, data = wcgs, ## dist = &quot;exponential&quot;) ## ## Estimates: ## est L95% U95% se ## rate 0.011089 0.009813 0.012531 0.000692 ## ## N = 3154, Events: 257, Censored: 2897 ## Total time at risk: 23176.25 ## Log-likelihood = -1413.964, df = 1 ## AIC = 2829.929 We recognize the estimate of \\(\\lambda\\). However, if we apply the coef() function we get coef(fit.e.ph) ## [1] -4.501807 and this is \\(\\log(\\lambda)\\) or \\(\\lambda=\\exp(-4.501807)\\). The fact that \\[ S(t) = \\exp(-H(t)) \\] shows that it is more natural to model the hazard or the cumulative hazard, and then estimate the survival. The cumulative hazard function should be in the focus during the modeling process. The survival function is then a by product. This is the approach taken when using the non-parametric Nelson-Aalen estimator of survival.First the cumulative hazard is estimated and then the survival. Let’s compare the non-parametric Nelson - Aalen estimate of the cumulative survival to the parametric exponential estimate. nelson.aalen.e.fit &lt;- nelsonaalen(wcgs,time169y,chd69) na.e.frame &lt;- data.frame(time=wcgs$time169y,H.na=nelson.aalen.e.fit, H.e = exp(coef(fit.e.ph))*wcgs$time169y) ggplot(na.e.frame,aes(x=time))+geom_line(aes(y=H.na,colour=&quot;blue&quot;))+ geom_line(aes(y=H.e,colour=&quot;green&quot;)) + xlab(&quot;Time (t)&quot;) + ylab(&quot;Cumulative Hazard H(t)&quot;) + scale_colour_manual(&quot;Estimate&quot;,values = c(&quot;blue&quot;,&quot;green&quot;),labels=c(&quot;N-Aalen&quot;,&quot;Exponential&quot;)) For completeness we also compare the survival curves based on the Nelson-Aalen estimate (\\(S(t)=exp(-H(t)\\)), the Kaplan Meier estimate, and the exponential estimate (\\(S(t)=exp(-\\lambda t)\\)). kmfit &lt;- survfit(Surv(time169y,chd69)~1,data=wcgs) km.frame &lt;- as.data.frame(summary(kmfit)[2:10]) ggplot(na.e.frame,aes(x=time))+geom_line(aes(y=exp(-H.na),colour=&quot;blue&quot;)) + geom_line(aes(y=exp(-H.e),colour=&quot;green&quot;)) + geom_step(data=km.frame,aes(time,surv,color=&quot;red&quot;),lty=2) + xlab(&quot;Time (t)&quot;) + ylab(&quot;Survival S(t)&quot;) + scale_colour_manual(&quot;Estimate&quot;,values = c(&quot;blue&quot;,&quot;green&quot;,&quot;red&quot;),labels=c(&quot;N-Aalen&quot;,&quot;Exponential&quot;,&quot;K-M&quot;)) 5.3 The Weibull survival function 5.3.1 Proportional hazards representation - PH A more flexible function for the hazard is based on the Weibull distribution. The hazard is then a non-constant function of time and has the form: \\[ h(t) = \\mu \\alpha t ^ {\\alpha - 1} \\] The cumulative hazard is then \\[ H(t) = \\mu t ^ {\\alpha } \\] and the survival \\[ S(t)=\\exp(-H(t))=\\exp(- \\mu t ^ \\alpha) \\] Note that \\(\\mu\\) and \\(\\alpha\\) are parameters and \\(t\\) stands for time. We refer to \\(\\alpha\\) as the the shape and \\(\\mu\\) as the scale. Note that when \\(\\alpha=1\\) we have the exponential distribution. Please note that the parametric form differs between textbooks and statistical programs and even between packages within R. Read the help files carefully! 5.3.2 The accelerated failure time representation - AFT In R the convention is to use the same form as for the Weibull distribution functions, dweibull etc. This is called the the accelerated failure time (AFT) representation. The cumulative hazard is: \\[ H(t) = (t/b)^a = \\frac{1}{b^a} t ^ a \\] and the hazard is \\[ h(t) = \\frac{a}{b^a} t ^ {a-1} \\] Note that \\(a\\) and \\(b\\) are parameters and \\(t\\) stands for time. The parameters in R (and other programs) have special names: \\(a\\) is called shape \\(b\\) is called scale Note that \\(b\\) scales the time and \\(a\\) controls the shape. The role of the shape is similar for the PH and AFT representations. Let’s try plotting the hazard and cumulative hazard for a given set of \\(a\\) and \\(b\\) in R. You can experiment by changing the values for \\(a\\) and \\(b\\) and study how the functional form changes. Note that we use \\(x\\) instead of \\(t\\) inside the function because \\(t\\) is reserved in R. The cumulative hazard function corresponding is below the corresponding hazard function. a&lt;-1 b&lt;-2 x &lt;- seq(0.1,10,0.1) h &lt;- function(x) {a/b^a * x ^ (a-1)} H &lt;- function(x) {(x/b)^a} par(mfcol=c(2,2)) plot(x,h(x),type=&quot;l&quot;,xlab=&quot;t&quot;,ylab=&quot;h(t)=a/b^a * t ^ (a-1)&quot;) text(6,jitter(mean(h(x))),labels=paste(&quot;a =&quot;,a,&quot; b =&quot;,b)) plot(x,H(x),type=&quot;l&quot;,xlab=&quot;t&quot;,ylab=&quot;H(t)=1/b^a * t ^ a&quot;) text(6,jitter(mean(H(x))),labels=paste(&quot;a =&quot;,a,&quot; b =&quot;,b)) a &lt;- 0.5 b &lt;- 1 plot(x,h(x),type=&quot;l&quot;,xlab=&quot;t&quot;,ylab=&quot;h(t)=a/b^a * t ^ (a-1)&quot;) text(6,jitter(mean(h(x))),labels=paste(&quot;a =&quot;,a,&quot; b =&quot;,b)) plot(x,H(x),type=&quot;l&quot;,xlab=&quot;t&quot;,ylab=&quot;H(t)=1/b^a * t ^ a&quot;) text(6,jitter(mean(H(x))),labels=paste(&quot;a =&quot;,a,&quot; b =&quot;,b)) par(mfrow=c(1,1)) 5.4 Estimating the hazard function and survival 5.4.1 Exponential with flexsurv fit.e.ph &lt;- flexsurvreg(Surv(time169y,chd69)~1,data=wcgs,dist=&quot;exponential&quot;) fit.e.ph ## Call: ## flexsurvreg(formula = Surv(time169y, chd69) ~ 1, data = wcgs, ## dist = &quot;exponential&quot;) ## ## Estimates: ## est L95% U95% se ## rate 0.011089 0.009813 0.012531 0.000692 ## ## N = 3154, Events: 257, Censored: 2897 ## Total time at risk: 23176.25 ## Log-likelihood = -1413.964, df = 1 ## AIC = 2829.929 coef(fit.e.ph) ## [1] -4.501807 5.4.2 Weibull PH with flexsurv We’ll use flexsurvreg to estimate \\(\\mu\\) and \\(\\alpha\\) for the wcgs data. Note the use of dist=“weibullph”. fit.w.ph &lt;- flexsurvreg(Surv(time169y,chd69)~1,data=wcgs,dist=&quot;weibullph&quot;) fit.w.ph ## Call: ## flexsurvreg(formula = Surv(time169y, chd69) ~ 1, data = wcgs, ## dist = &quot;weibullph&quot;) ## ## Estimates: ## est L95% U95% se ## shape 1.29985 1.15471 1.46324 0.07853 ## scale 0.00601 0.00429 0.00843 0.00104 ## ## N = 3154, Events: 257, Censored: 2897 ## Total time at risk: 23176.25 ## Log-likelihood = -1405.351, df = 2 ## AIC = 2814.703 coef(fit.w.ph) ## shape scale ## 0.2622523 -5.1140774 This means that \\[ H(t) = H_0(t) = \\exp(-5.1141) t ^ {\\exp(0.2622523)} \\] Here we write \\(\\mu=\\exp(\\beta_0)\\) where \\(\\beta_0\\) is the log-scale from the flexsurv model. Let’s draw the \\(h()\\) from the exponential estimate (broken line) and the Weibull estimate (blue line). We use \\(x\\) for time because \\(t\\) is a reserved function in R. R computes for for each value in \\(x\\) the value of \\(h(x)\\) where we take \\(\\mu\\) and \\(\\alpha\\) from the estimation above. plot(fitm,ylim=c(0,0.03)) x &lt;- seq(0,9,0.1) alpha&lt;-exp(coef(fit.w.ph)[1]) mu&lt;-exp(coef(fit.w.ph)[2]) h &lt;- function(x) {mu*alpha*x^(alpha-1)} lines(x,h(x),col=&quot;blue&quot;) abline(h=exp(coef(fit.e.ph)),lty=2,col=&quot;green&quot;) It is easier to estimate the cumulative hazard or \\(H(t)\\) because it is a smooth function. The hazard function can be unstable when there are few events. Therefore it is more common to compare cumulative hazards function between models. We plot the Nelons-Aalen non-parametric estimate of the cumulative hazard. Then we superimpose the Weibull estimate, and the exponential estimate. Here the exponential is \\(H_E(t) = \\lambda t\\) and the Weibull is \\(H_W(t) = \\mu t ^ \\alpha\\). nelson.aalen &lt;- nelsonaalen(wcgs,time169y,chd69) plot(wcgs$time169y,nelson.aalen,xlab=&quot;Time (t)&quot;,ylab=&quot;Cumulative Hazard H(t)&quot;) x &lt;- seq(0,9,0.1) alpha&lt;-exp(coef(fit.w.ph)[1]) mu&lt;-exp(coef(fit.w.ph)[2]) H &lt;- function(x) {mu*x^alpha} lines(x,H(x),col=&quot;green&quot;,lwd=2) lines(x,exp(coef(fit.e.ph))*x,lwd=2,lty=2,col=&quot;blue&quot;) The Weibull version is much closer to the Nelson-Aalen estimate than the exponential. Note that the exponential is a special case of the Weibull when \\(\\alpha=1\\). Therefore if log(shape)=0 or shape=1, then the Weibull becomes an exponential. From the output we see the test for log(shape)=0. We can also compare log-likelihood values, and show that below. We can use a likelihood ratio test to compare the Weibull and the exponential. The difference is 1 parameter so we compare the 2 times difference in log likelihoods to the chi-square distribution with 1 degree of freedom. qchisq(0.95,1) ## [1] 3.841459 The difference is greater than the reference value from the chi-square distribution, and therefore we conclude that the Weibull model gives a better fit. -2*(fit.e.ph$loglik - fit.w.ph$loglik) ## [1] 17.2259 5.5 Covariates and Hazard ratios Consider the difference in hazard between the behavior types A and B. From a Poisson model we see that the rate ratio is 2.34. futpb &lt;- tapply(wcgs$time169y,wcgs$dibpat0f,sum) enpb &lt;- tapply(wcgs$chd69,wcgs$dibpat0f,sum) Number of events per 1000 person years by type: lambda&lt;-enpb/futpb*1000 lambda ## B A ## 6.691579 15.654735 lambda[2]/lambda[1] ## A ## 2.339468 fitdb &lt;- glm(chd69 ~ dibpat0f+offset(log(time169y)),data=wcgs,family=poisson()) summary(fitdb) ## ## Call: ## glm(formula = chd69 ~ dibpat0f + offset(log(time169y)), family = poisson(), ## data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.5415 -0.5023 -0.3360 -0.3251 3.7049 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.0069 0.1125 -44.502 &lt; 2e-16 *** ## dibpat0fA 0.8499 0.1352 6.287 3.24e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 1656.5 on 3153 degrees of freedom ## Residual deviance: 1613.5 on 3152 degrees of freedom ## AIC: 2131.5 ## ## Number of Fisher Scoring iterations: 6 exp(coef(fitdb)[2]) ## dibpat0fA ## 2.339468 exp(confint.default(fitdb))[2,] ## 2.5 % 97.5 % ## 1.794917 3.049228 From the exponential (\\(\\alpha=1\\) or shape=1) we get: fit.w1.ph.m1 &lt;- flexsurvreg(Surv(time169y,chd69)~dibpat0f, dist=&quot;exponential&quot;,data=wcgs) fit.w1.ph.m1 ## Call: ## flexsurvreg(formula = Surv(time169y, chd69) ~ dibpat0f, data = wcgs, ## dist = &quot;exponential&quot;) ## ## Estimates: ## data mean est L95% U95% se exp(est) ## rate NA 0.006692 0.005367 0.008343 0.000753 NA ## dibpat0fA 0.503805 0.849924 0.584957 1.114890 0.135190 2.339468 ## L95% U95% ## rate NA NA ## dibpat0fA 1.794914 3.049233 ## ## N = 3154, Events: 257, Censored: 2897 ## Total time at risk: 23176.25 ## Log-likelihood = -1392.488, df = 2 ## AIC = 2788.977 From the Weibull we get: fit.w.ph.m1 &lt;- flexsurvreg(Surv(time169y,chd69)~dibpat0f, dist=&quot;weibullph&quot;, data=wcgs) fit.w.ph.m1 ## Call: ## flexsurvreg(formula = Surv(time169y, chd69) ~ dibpat0f, data = wcgs, ## dist = &quot;weibullph&quot;) ## ## Estimates: ## data mean est L95% U95% se exp(est) ## shape NA 1.306045 1.160702 1.469588 0.078616 NA ## scale NA 0.003570 0.002426 0.005253 0.000703 NA ## dibpat0fA 0.503805 0.856917 0.591934 1.121900 0.135198 2.355886 ## L95% U95% ## shape NA NA ## scale NA NA ## dibpat0fA 1.807481 3.070682 ## ## N = 3154, Events: 257, Censored: 2897 ## Total time at risk: 23176.25 ## Log-likelihood = -1383.514, df = 3 ## AIC = 2773.028 From the Weibull with age adjustment we get (here agec=age0-46): fit.w.ph.m2 &lt;- flexsurvreg(Surv(time169y,chd69)~dibpat0f+agec, dist=&quot;weibullph&quot;, data=wcgs) fit.w.ph.m2 ## Call: ## flexsurvreg(formula = Surv(time169y, chd69) ~ dibpat0f + agec, ## data = wcgs, dist = &quot;weibullph&quot;) ## ## Estimates: ## data mean est L95% U95% se exp(est) ## shape NA 1.314663 1.168553 1.479041 0.079025 NA ## scale NA 0.003338 0.002261 0.004929 0.000664 NA ## dibpat0fA 0.503805 0.795310 0.529457 1.061162 0.135642 2.215126 ## agec 0.278694 0.066254 0.045257 0.087251 0.010713 1.068498 ## L95% U95% ## shape NA NA ## scale NA NA ## dibpat0fA 1.698010 2.889727 ## agec 1.046297 1.091171 ## ## N = 3154, Events: 257, Censored: 2897 ## Total time at risk: 23176.25 ## Log-likelihood = -1364.788, df = 4 ## AIC = 2737.577 Be sure to understand the parametric form of the flexsurvreg function. In general we have \\[ h(t) = \\mu \\alpha t ^ {\\alpha-1} \\exp(\\beta_1 x_1 + \\cdots + \\beta_p x_p), \\] where \\(x_1,\\ldots,x_p\\) are explanatory variables, and \\(h_0(t)=\\mu \\alpha t ^ {\\alpha-1}\\). Sometimes we write \\(\\mu=exp(\\beta_0)\\) and then \\[ h(t) =\\alpha t ^ {\\alpha-1} \\exp(\\beta_0 +\\beta_1 x_1 + \\cdots + \\beta_p x_p), \\] Consider and example. Let \\(x\\) be a dummy explanatory variable for the behavior A or B. Let \\(x_1=1\\) if the type is A and \\(x_1=0\\) if the type is B. Let \\(x_2=0\\) meaning that agem=0 or age0=46. For B of age 46 (the reference group) we get: \\[ h_B(t)=h_0(t) = \\mu \\alpha t ^ {a-1}. \\] For A of age 46 we get \\[ h_A(t) =\\mu \\alpha t ^ {a-1} \\exp(\\beta_1). \\] The hazard ratio (Hazard Ratio = HR) for A vs B is \\[ HR = h_A(t) / h_B(t) = \\exp(\\beta_1). \\] Let’s consider the effect of age, where \\(x_2\\) is the age variable. Then \\(h(t)\\) for B of age \\(k\\) is \\[ h_B(t) = \\mu \\alpha t ^ {a-1} \\exp(\\beta_2 k), \\] and \\(h(t)\\) for an A-type, also of age \\(k\\) units from 46, we get \\[ h_A(t) = \\mu \\alpha t ^ {a-1} \\exp(\\beta_1 + \\beta_2 k). \\] The cumulative hazard for A given age of k units from 46 is: \\[ H_A(t) = \\mu t ^ {a} \\exp(\\beta_1 + \\beta_2 k). \\] The adjusted hazard ratio for A vs B is: \\[ HR = h_A(t) / h_B(t) = \\exp(\\beta_1). \\] The formula is the same, but the estimate of \\(\\beta_1\\) could be different if the hazard depends on age and there is an age difference between the groups (in other words, confounding is present). Recall that \\(exp(y)/exp(z) = exp(y-z)\\). The age variable cancels in hazard ratio computations because we assume the effect of age is same for A and B. We could study that by introducing interaction between age and behavior. Let’s plot the cumulative hazards for the A and B types. We show the Nelson-Aalen and the Weibull, with and without age adjustment. We have to use the order function to sort the time values. Be sure to understand the the form of \\(H_W(t)\\) for the A group. First we use the unadjusted Weibull model. wcgsA &lt;- subset(wcgs,dibpat0f==&quot;A&quot;) wcgsB &lt;- subset(wcgs,dibpat0f==&quot;B&quot;) nelson.aalenA &lt;- nelsonaalen(wcgsA,time169y,chd69) nelson.aalenB &lt;- nelsonaalen(wcgsB,time169y,chd69) oA &lt;- order(wcgsA$time169y) oB &lt;- order(wcgsB$time169y) plot(wcgsA$time169y[oA],nelson.aalenA[oA],xlab=&quot;Time&quot;,ylab=&quot;H(t)&quot;,type=&quot;l&quot;) lines(wcgsB$time169y[oB],nelson.aalenB[oB]) x &lt;- seq(0,9,0.1) # H_0(t) mu &lt;- exp(coef(fit.w.ph.m1)[2]) alpha &lt;- exp(coef(fit.w.ph.m1)[1]) H0 &lt;- function(x) {mu*x^alpha} lines(x,H0(x),col=&quot;blue&quot;,lwd=2) lines(x,H0(x) * exp(0.857),col=&quot;red&quot;,lwd=2) Now use age adjustment (with respect to age 46). Then agec=0. wcgsA &lt;- subset(wcgs,dibpat0f==&quot;A&quot;) wcgsB &lt;- subset(wcgs,dibpat0f==&quot;B&quot;) nelson.aalenA &lt;- nelsonaalen(wcgsA,time169y,chd69) nelson.aalenB &lt;- nelsonaalen(wcgsB,time169y,chd69) oA &lt;- order(wcgsA$time169y) oB &lt;- order(wcgsB$time169y) plot(wcgsA$time169y[oA],nelson.aalenA[oA],xlab=&quot;Time&quot;,ylab=&quot;H(t)&quot;,type=&quot;l&quot;) lines(wcgsB$time169y[oB],nelson.aalenB[oB]) x &lt;- seq(0,9,0.1) mu &lt;- exp(coef(fit.w.ph.m2)[2]) alpha &lt;- exp(coef(fit.w.ph.m2)[1]) H0 &lt;- function(x) {mu*x^alpha} lines(x,H0(x) ,col=&quot;blue&quot;,lwd=2) lines(x,H0(x) * exp(0.795),col=&quot;red&quot;,lwd=2) Note that the red line is below the un-adjusted estimate. This is the effect of the age adjustment. The A group was slightly older. The difference between the groups has now been adjusted for age. Mean age of the A and B types: tapply(wcgs$age0,wcgs$dibpat0f,mean) ## B A ## 45.78722 46.76274 Let’s inspect age adjustment to age 50. Then agec=4. wcgsA &lt;- subset(wcgs,dibpat0f==&quot;A&quot;) wcgsB &lt;- subset(wcgs,dibpat0f==&quot;B&quot;) nelson.aalenA &lt;- nelsonaalen(wcgsA,time169y,chd69) nelson.aalenB &lt;- nelsonaalen(wcgsB,time169y,chd69) oA &lt;- order(wcgsA$time169y) oB &lt;- order(wcgsB$time169y) plot(wcgsA$time169y[oA],nelson.aalenA[oA],xlab=&quot;Time&quot;,ylab=&quot;H(t)&quot;,type=&quot;l&quot;) lines(wcgsB$time169y[oB],nelson.aalenB[oB]) x &lt;- seq(0,9,0.1) # H_W(t) lines(x,H0(x) * exp(0.066*4),col=&quot;blue&quot;,lwd=2) lines(x,H0(x) * exp(0.795 + 0.066*4),col=&quot;red&quot;,lwd=2) The lines are now above the un-adjusted lines. These are so called model based survival curves. We prefer conditional or standardized survival curves. However, this has not been implemented for flexsurvreg. 5.6 Test of interaction using flexurvreg Is there an interaction between age and type? fit.w.ph.m3 &lt;- flexsurvreg(Surv(time169y,chd69)~dibpat0f+agec+dibpat0f:agec,dist=&quot;weibull&quot;,data=wcgs) fit.w.ph.m3 ## Call: ## flexsurvreg(formula = Surv(time169y, chd69) ~ dibpat0f + agec + ## dibpat0f:agec, data = wcgs, dist = &quot;weibull&quot;) ## ## Estimates: ## data mean est L95% U95% se exp(est) ## shape NA 1.3149 1.1688 1.4793 0.0790 NA ## scale NA 75.8117 55.1061 104.2971 12.3386 NA ## dibpat0fA 0.5038 -0.5897 -0.8140 -0.3654 0.1145 0.5545 ## agec 0.2787 -0.0451 -0.0750 -0.0152 0.0153 0.9559 ## dibpat0fA:agec 0.3843 -0.0075 -0.0426 0.0276 0.0179 0.9925 ## L95% U95% ## shape NA NA ## scale NA NA ## dibpat0fA 0.4431 0.6939 ## agec 0.9277 0.9849 ## dibpat0fA:agec 0.9583 1.0280 ## ## N = 3154, Events: 257, Censored: 2897 ## Total time at risk: 23176.25 ## Log-likelihood = -1364.7, df = 5 ## AIC = 2739.401 The difference in -2 times likelihood is ll &lt;- -2*(logLik(fit.w.ph.m2) - logLik(fit.w.ph.m3)) ll ## &#39;log Lik.&#39; 0.1759378 (df=4) The p-value is 1-pchisq(ll,1) ## &#39;log Lik.&#39; 0.6748878 (df=4) The interaction term is not statistically significant. Therefore we do not have evidence for an interaction, so the effect of age does not depend on the behavior type. We drop the term dibpat0f:agem from the model. 5.7 Comparison with a Cox model fit.cox.ph.m3 &lt;- coxph(Surv(time169y,chd69)~dibpat0f+agec+dibpat0f:agec, data=wcgs) fit.cox.ph.m3 ## Call: ## coxph(formula = Surv(time169y, chd69) ~ dibpat0f + agec + dibpat0f:agec, ## data = wcgs) ## ## coef exp(coef) se(coef) z p ## dibpat0fA 0.76521 2.14944 0.14362 5.328 9.92e-08 ## agec 0.06024 1.06209 0.01977 3.047 0.00232 ## dibpat0fA:agec 0.01064 1.01070 0.02355 0.452 0.65135 ## ## Likelihood ratio test=81.97 on 3 df, p=&lt; 2.2e-16 ## n= 3154, number of events= 257 The interaction is not signficant. The reduced model is fit.cox.ph.m2 &lt;- update(fit.cox.ph.m3,.~.-dibpat0f:agec) fit.cox.ph.m2 ## Call: ## coxph(formula = Surv(time169y, chd69) ~ dibpat0f + agec, data = wcgs) ## ## coef exp(coef) se(coef) z p ## dibpat0fA 0.78679 2.19634 0.13567 5.799 6.65e-09 ## agec 0.06772 1.07007 0.01074 6.306 2.86e-10 ## ## Likelihood ratio test=81.76 on 2 df, p=&lt; 2.2e-16 ## n= 3154, number of events= 257 Comparing the HRs data.frame(cox=exp(coef(fit.cox.ph.m2)),weibull=exp(coef(fit.w.ph.m2)[3:4])) ## cox weibull ## dibpat0fA 2.196337 2.215126 ## agec 1.070068 1.068498 Add the Cox estimates wcgsA &lt;- subset(wcgs,dibpat0f==&quot;A&quot;) wcgsB &lt;- subset(wcgs,dibpat0f==&quot;B&quot;) nelson.aalenA &lt;- nelsonaalen(wcgsA,time169y,chd69) nelson.aalenB &lt;- nelsonaalen(wcgsB,time169y,chd69) oA &lt;- order(wcgsA$time169y) oB &lt;- order(wcgsB$time169y) plot(wcgsA$time169y[oA],nelson.aalenA[oA],xlab=&quot;Time&quot;,ylab=&quot;H(t)&quot;,type=&quot;l&quot;) lines(wcgsB$time169y[oB],nelson.aalenB[oB]) x &lt;- seq(0,9,0.1) mu &lt;- exp(coef(fit.w.ph.m2)[2]) alpha &lt;- exp(coef(fit.w.ph.m2)[1]) H0 &lt;- function(x) {mu*x^alpha} lines(x,H0(x) ,col=&quot;blue&quot;,lwd=2) lines(x,H0(x) * exp(0.795),col=&quot;red&quot;,lwd=2) # Baseline Cox coxB &lt;- survfit(fit.cox.ph.m2,newdata = data.frame(agec=0,dibpat0f=&quot;B&quot;)) lines(coxB$time,coxB$cumhaz,col=&quot;green&quot;) lines(coxB$time,coxB$cumhaz*exp(coef(fit.cox.ph.m2)[1]),col=&quot;orange&quot;) 5.8 Estimating risk What is the 5 year risk of CHD for type A who is 49 years of age? First we define the risk function. chd.risk &lt;- function(t,wfit,vars) { mu &lt;- exp(coef(wfit)[2]) alpha &lt;- exp(coef(wfit)[1]) H0 &lt;- function(x) {mu*x^alpha} S0 &lt;- function(x) {exp(-H0(x))} H &lt;- H0(t) * exp(sum(vars*coef(wfit)[-c(1:2)])) 1-S0(t)^exp(sum(vars*coef(wfit)[-c(1:2)])) } Then we have to supply the appropriate covariate values. Note that 0 means type B and 1 means type A. According to the Weibull model we have for type A of age 46.8 and 49 (49-46=3): chd.risk(5,fit.w.ph.m2,c(1,0.8)) ## scale ## 0.06264133 chd.risk(5,fit.w.ph.m2,c(1,3)) ## scale ## 0.07210822 According to the Weibull model we have for type B of age 45.8 and 49 (49-46=3): chd.risk(5,fit.w.ph.m2,c(0,-.2)) ## scale ## 0.02696116 chd.risk(5,fit.w.ph.m2,c(0,3)) ## scale ## 0.03322159 From a Kaplan Meier model we get the unadjusted values for each type as: km.fit &lt;- survfit(Surv(time169y,chd69)~dibpat0f,data=wcgs) 1-summary(km.fit,times=5)$surv ## [1] 0.02512947 0.06592613 We can also create the risk function for the Cox model using basehaz. We use approxfun to build a function based on the results from basehaz. chd.risk.cox &lt;- function(t,cfit,vars) { bs.cox &lt;- basehaz(cfit,centered = FALSE) H0 &lt;- approxfun(bs.cox$time,bs.cox$hazard) S0 &lt;- function(t) {exp(-H0(t))} 1-S0(t) ^ exp(sum(vars*coef(cfit))) } According to the Cox model we have for type B of age 45.8 and 49 (49-46=3): chd.risk.cox(5,fit.cox.ph.m2,c(0,-.2)) ## [1] 0.02580958 chd.risk.cox(5,fit.cox.ph.m2,c(0,3)) ## [1] 0.03195436 All 2-way interactions fit.cox.all2 &lt;- coxph(Surv(time169y,chd69)~(age0+sbp0+smoker+dibpat0f)^2,data=wcgs) drop1(fit.cox.all2,test=&quot;Chisq&quot;) ## Single term deletions ## ## Model: ## Surv(time169y, chd69) ~ (age0 + sbp0 + smoker + dibpat0f)^2 ## Df AIC LRT Pr(&gt;Chi) ## &lt;none&gt; 3909.1 ## age0:sbp0 1 3907.8 0.61683 0.43223 ## age0:smoker 1 3907.8 0.66769 0.41386 ## age0:dibpat0f 1 3907.5 0.32199 0.57041 ## sbp0:smoker 1 3907.1 0.00473 0.94517 ## sbp0:dibpat0f 1 3908.9 1.77936 0.18223 ## smoker:dibpat0f 1 3909.9 2.74634 0.09748 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We do not have any statistically signficant interactions here. "],
["time-dependent-variables-and-covariates.html", "Chapter 6 Time-Dependent Variables and Covariates", " Chapter 6 Time-Dependent Variables and Covariates asdad "],
["competing-risks.html", "Chapter 7 Competing Risks", " Chapter 7 Competing Risks asdad "]
]
